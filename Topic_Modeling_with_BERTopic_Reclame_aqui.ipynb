{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/Topic_Modeling_with_BERTopic_Reclame_aqui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modeling with BERTopic - Reclame Aqui**\n",
        "\n",
        "BERTopic is a topic modeling technique that leverages transformers and a custom class-based TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions \n",
        "\n",
        "Reference: (https://maartengr.github.io/BERTopic/index.html)."
      ],
      "metadata": {
        "id": "VG5Z-hvZ9GIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enabling the GPU**\n",
        "\n",
        "We will use the GPU provided by COLAB to accelarate our model training. To enable GPUs for the notebook:\n",
        "1- Navigate to Edit -> Notebook Settings\n",
        "2- Select GPU from the Hardware Accelerator drop-down"
      ],
      "metadata": {
        "id": "W_0vnbvROpuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify if GPU is enable\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "gj08UQrDOqW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ed5aa5-1fa6-4f44-ad52-e1f334f80c28"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan  6 20:30:46 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    34W /  70W |   3908MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**"
      ],
      "metadata": {
        "id": "eXD8VBix7DD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyspellchecker\n",
        "!pip install bertopic\n",
        "!pip install kaleido # for save BERTopic plots as image"
      ],
      "metadata": {
        "id": "csLlF-UUb2il"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "CQ0hDdDm6gtj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # for data manipulation\n",
        "import os # for interacting with the operating system\n",
        "import nltk # for natural language processing\n",
        "import string # for string manipulation\n",
        "import re # for for regular expressions\n",
        "import matplotlib.pyplot as plt # for visualization\n",
        "import spacy # for lemmatize portuguese text\n",
        "from bertopic import BERTopic # for topic modeling\n",
        "from spellchecker import SpellChecker # for spell check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install spacy pt_core_news_sm for portuguese text\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "dafLrQKmvlGD"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "BvRCi7Cvp0G6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73411fb-7408-4c52-df72-bfb8a6f1089b"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset with stopwords\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "JAvaPg5ep3Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234b1cba-ceb9-4542-ab6e-80fa9e1ff4fe"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload datasets for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "97RrmPshvcmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d7b178-0603-428f-a4ba-92b9d070a969"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload dependency need to stem portuguese text\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "id": "qn9OrF3EXCNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467c7452-4ad6-449d-ad82-c5fa5c16ed64"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load data from [Github](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui.git)**"
      ],
      "metadata": {
        "id": "EGjKdT4C8RVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/punkmic/Topic-Modeling-Reclame-Aqui.git"
      ],
      "metadata": {
        "id": "C_yss1JC-8Ou"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory\n",
        "%cd /content/Topic-Modeling-Reclame-Aqui \n",
        "\n",
        "# Update files from remote repository\n",
        "!git pull \n",
        "\n",
        "# Return to work directory\n",
        "%cd ..\n",
        "\n",
        "# Check current directory\n",
        "!pwd"
      ],
      "metadata": {
        "id": "uv3QBnDRHFPv",
        "outputId": "f3a6153b-3198-4357-d016-d72084da0519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Topic-Modeling-Reclame-Aqui\n",
            "Warning: Permanently added the RSA host key for IP address '140.82.112.4' to the list of known hosts.\n",
            "Already up to date.\n",
            "/content\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path_csv, drop_duplicates = True, lower=True):\n",
        " \n",
        "  # use the read_csv method to read csv file\n",
        "  df = pd.read_csv(path_csv)\n",
        "  \n",
        "  if drop_duplicates:\n",
        "    # read and return the CSV file using the read_csv method\n",
        "    print(f\"Shape before remove duplicates: {df.shape}\")\n",
        "\n",
        "    # use the drop_duplicated method to drop duplicates rows\n",
        "    df = df.drop_duplicates(subset=\"text\")\n",
        "\n",
        "    print(f\"Shape after remove duplicates: {df.shape}\")\n",
        "\n",
        "    if lower:\n",
        "      # apply the str.lower() method to each element in the dataframe\n",
        "      df = df.applymap(str.lower)\n",
        "    \n",
        "     # rename columns\n",
        "    df.columns = [\"title\", \"documents\"] \n",
        "\n",
        "    # use the replace() method to replace the string with an empty string\n",
        "    df = df.replace(re.compile('\\[editado pelo reclame aqui\\]|editado pelo reclame aqui|Editado pelo Reclame Aqui'), '')\n",
        "    df = df.replace(re.compile('\\[casas bahia\\]|Casa Bahia|Casas Bahia|casa bahia'), '')\n",
        "    df = df.replace(re.compile('\\[magazine luiza\\]|Magazine luiza|Magazine Luiza| Magazine luizar|Magazine Luizar'), '')\n",
        "    df = df.replace(re.compile('\\[mercado livre\\]|Mercado Livre|Mercado livre'), '')\n",
        "    df = df.replace(re.compile('\\[americana\\]|Ameriacanas|ameriacanas'), '')\n",
        "\n",
        "    # drop the old index column\n",
        "    df.reset_index(inplace = True, drop = True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "F3IoOKARBwVb"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing**"
      ],
      "metadata": {
        "id": "drjuwA5yk4YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tokenization**\n",
        "\n",
        "Tokenization aims to breaking text down into its component parts"
      ],
      "metadata": {
        "id": "i9ukWCy8qBOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_TOKENIZER = nltk.tokenize.word_tokenize\n",
        "def tokenize(text):\n",
        "  tokens = [token.strip().lower() for token in WORD_TOKENIZER(text, language=\"portuguese\")]\n",
        " \n",
        "  # set a pattern to detect patterns such as x x, xxx x, xxx xxx\n",
        "  pattern = r\"\\b\\w+\\s+\\w+\\b\"\n",
        " \n",
        "  # filter tokens by pattern\n",
        "  filtered_words = [word for word in tokens if re.search(pattern, word)]\n",
        "\n",
        "  # return token if not in filter list\n",
        "  return [token for token in tokens if token not in filtered_words]"
      ],
      "metadata": {
        "id": "k_f8hsxJk7v4"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Stem** \n",
        "\n",
        "Stem the tokens. This step aims to remove morphological affixes and normalize to standardized stem forms"
      ],
      "metadata": {
        "id": "Lm7V--yMq98r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEMMER = nltk.stem.RSLPStemmer()\n",
        "def stem(tokens):\n",
        "  return [STEMMER.stem(token) for token in tokens]"
      ],
      "metadata": {
        "id": "TphgFQIxrPMM"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Lemmatize**\n",
        "\n",
        "Lemmatize the tokens. Retains more natural forms than stemming, but assumes all tokens nons unless tokens are passed as (word, pos) tuples. Note: nltk lemmatize does not suport portugues language"
      ],
      "metadata": {
        "id": "RTiUcqzrrdiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEMMATIZER = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(tokens):\n",
        "  lemmas = []\n",
        "  for token in tokens:\n",
        "      if isinstance(token, str):\n",
        "          # treats token like a noun\n",
        "          lemmas.append(LEMMATIZER.lemmatize(token)) \n",
        "      else: \n",
        "          # assume a tuple of (word, pos)\n",
        "          lemmas.append(LEMMATIZER.lemmatize(*token))\n",
        "  return lemmas"
      ],
      "metadata": {
        "id": "4O48ihphrxy3"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatize option for portuguese text**"
      ],
      "metadata": {
        "id": "eHdBPARlU24i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load portuguese model\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "def lemmatize_pt(tokens):\n",
        "\n",
        "  # Create a spaCy Doc object and apply the lemmatization\n",
        "  doc = nlp(' '.join(tokens))\n",
        "\n",
        "  # Return lemmatize\n",
        "  return [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "pfgdVUHVU6Zy"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove stopwords**\n",
        "\n",
        "Stop words are things like articles and conjunctions that usually do not offer a lot of value in an analysis."
      ],
      "metadata": {
        "id": "jkLYqJgDs4Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(tokens, stopwords=None, custom_stop_words = None):\n",
        "\n",
        "  if custom_stop_words is None:\n",
        "    custom_stop_words = ['amazon', 'americanas', 'casas bahia', 'magazine luiza', 'shein', 'kabum',\n",
        "                       'samsung', 'mercado livre', 'banco brasil', 'apple', 'magazine', 'luiza', 'luizar',\n",
        "                      'casas', 'bahia', 'casa', 'mercado', 'livre']\n",
        "\n",
        "  # Use the default stop words if none is passed\n",
        "  if stopwords is None:\n",
        "    stopwords = nltk.corpus.stopwords.words('portuguese') + custom_stop_words\n",
        "  \n",
        "  # Filter the list of tokens to exclude the stop word tokens\n",
        "  return [token for token in tokens if token not in stopwords]"
      ],
      "metadata": {
        "id": "D9blZaBYtHgj"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stop_words = ['amazon', 'americanas', 'casas bahia', 'magazine luiza', 'shein', 'kabum',\n",
        "                       'samsung', 'mercado livre', 'banco brasil', 'apple', 'magazine', 'luiza', 'luizar',\n",
        "                      'casas', 'bahia', 'casa', 'mercado', 'livre']\n",
        "assert remove_stopwords(['compra', 'echar', 'em esse', 'amazon', 'pude'], custom_stop_words=custom_stop_words) == ['compra', 'echar', 'em esse', 'pude']"
      ],
      "metadata": {
        "id": "-E4DqCin2HDc"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove hyperlinks**\n",
        "\n",
        "Removes http/s links from the tokens."
      ],
      "metadata": {
        "id": "X4qMKyfKtaAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_links(tokens):\n",
        "  # Filter tokens that starts with \"http://\" or \"https://\"\n",
        "  return [token for token in tokens \n",
        "          if not token.startswith(\"http://\")\n",
        "          and not token.startswith(\"https://\")]"
      ],
      "metadata": {
        "id": "LqlMta5qtovS"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_links(['bom', 'http://online', 'https://offline']) == ['bom']"
      ],
      "metadata": {
        "id": "vxvsG7s_6wkd"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove numbers**"
      ],
      "metadata": {
        "id": "EwKFluprz-pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(tokens):\n",
        "  # Use a regular expression to match words that contain numbers\n",
        "  pattern = r\"\\b\\w*\\d\\w*\\b\"\n",
        "  tokens = [token for token in tokens if not re.sub(pattern, \"\", token) == \"\"]\n",
        "  \n",
        "  # Filter out number tokens using a list comprehension and the isnumeric method\n",
        "  return [token for token in tokens if not token.isnumeric()]"
      ],
      "metadata": {
        "id": "ALKnXv8C0EWu"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_numbers(['ola', 'bicicleta', '1', '2002']) == ['ola', 'bicicleta']"
      ],
      "metadata": {
        "id": "32uzYfBG3cv2"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove date**"
      ],
      "metadata": {
        "id": "a0pZKAQ7ZJHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_date(tokens):\n",
        "  # Compile a regular expression to match dates in the format dd/mm or dd/mm/yyyy\n",
        "  date_regex = re.compile(r'\\d{2}/\\d{2}(/\\d{4})?')\n",
        "\n",
        "  # Use the regex to find all the tokens that match the date pattern\n",
        "  dates = [token for token in tokens if date_regex.fullmatch(token)]\n",
        "\n",
        "  # Filter the list of tokens to exclude the date tokens\n",
        "  filtered_tokens = [token for token in tokens if token not in dates]\n",
        "\n",
        "  # Return the filtered tokens\n",
        "  return filtered_tokens"
      ],
      "metadata": {
        "id": "ywykIjKZctc-"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_date(['texto', 'data', '20/10', 'seguro', '02/09/2014']) == ['texto', 'data', 'seguro']"
      ],
      "metadata": {
        "id": "k6r3cMbB7F3f"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove punctuation**"
      ],
      "metadata": {
        "id": "7y1FqCD1t75_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(tokens,\n",
        "                       strip_mentions=True,\n",
        "                       strip_hashtags=True):\n",
        "\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "\n",
        "    # Filter punctuation tokens\n",
        "    tokens = [token.strip() for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Remove @ symbol from left side of tokens\n",
        "    if strip_mentions:\n",
        "        tokens = [t.lstrip(r\"([!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~])\\1+\") for t in tokens]\n",
        "\n",
        "    # Remove # symbol from left side of tokens\n",
        "    if strip_hashtags:\n",
        "        tokens = [t.lstrip(r\"([!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~])\\1+\") for t in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "ba7paG7_uBtS"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_punctuation(['limpo', 'acento/  ///', 'simples???', 'onde', ',']) == ['limpo', 'acento', 'simples', 'onde']"
      ],
      "metadata": {
        "id": "7E8n6Z0l7Xe_"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove short tokens**"
      ],
      "metadata": {
        "id": "SiUx2T8C5YBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_tokens(tokens):\n",
        "  # Filter the list of tokens to exclude tokens that are shorter than four letters\n",
        "  filtered_tokens = [token for token in tokens if len(token) >= 4]\n",
        "\n",
        "  # Return the filtered tokens\n",
        "  return filtered_tokens"
      ],
      "metadata": {
        "id": "t8SOwNYW6QI1"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_short_tokens(['sair', 'um', 'correto', 'igual', 'oi', 'de', 'em']) == ['sair', 'correto', 'igual']"
      ],
      "metadata": {
        "id": "GTF9de9N7uFK"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Correction of spelling errors**"
      ],
      "metadata": {
        "id": "WqtOisN38pZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SpellChecker object\n",
        "spell = SpellChecker(language='pt')\n",
        "\n",
        "def check_spell_errors(text):\n",
        "  result = []\n",
        "  for token in text:\n",
        "    # Correct the spelling errors in the text\n",
        "    corrected_text = spell.correction(token)\n",
        "\n",
        "    # If no correction is present user the original text\n",
        "    if corrected_text == None:\n",
        "      corrected_text =  token\n",
        "  \n",
        "    result.append(corrected_text)\n",
        "  # Return the corrected text\n",
        "  return result"
      ],
      "metadata": {
        "id": "fUTiJYc_8hba"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove extra white spaces**"
      ],
      "metadata": {
        "id": "ybMjuBaSw67b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_whitespace(document):\n",
        "    return  \" \".join(document.split())"
      ],
      "metadata": {
        "id": "3ig6LBnhw_6L"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df, colname):\n",
        "  df[colname]= df[colname].str.lower()\n",
        "  df[colname]= df[colname].apply(remove_whitespace)\n",
        "  df['tokenize'] = df[colname].apply(tokenize)\n",
        "  #df['spell_error'] = df['tokenize'].apply(check_spell_errors)\n",
        "  df['remove_links'] = df['tokenize'].apply(remove_links)\n",
        "  df['remove_punctuation'] = df['remove_links'].apply(remove_punctuation)\n",
        "  df['remove_numbers'] = df['remove_punctuation'].apply(remove_numbers)\n",
        "  df['remove_date'] = df['remove_numbers'].apply(remove_date)\n",
        "  df['remove_short_tokens'] = df['remove_date'].apply(remove_short_tokens)\n",
        "  df['remove_stop_words'] = df['remove_short_tokens'].apply(remove_stopwords)\n",
        "  df['lemmas'] = df['remove_stop_words'].apply(lemmatize_pt) \n",
        "  df['corpus'] = df['lemmas'].apply(lambda x: ' '.join(x))\n",
        "  return df"
      ],
      "metadata": {
        "id": "Tu75mPgywHwb"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(features):\n",
        "  dfs = {}\n",
        "  # create models for documents and title features\n",
        "  for feature in features:\n",
        "    df = None\n",
        "    if not os.path.exists(f'/content/Topic-Modeling-Reclame-Aqui/results/dataset/preprocessed/table_{feature}.csv'):\n",
        "\n",
        "      path_csv = \"/content/Topic-Modeling-Reclame-Aqui/corpus.csv\"\n",
        "  \n",
        "      if df is None:\n",
        "        # read data \n",
        "        df = read_data(path_csv)\n",
        "  \n",
        "      # preprocess data\n",
        "      df = preprocess(df, feature)\n",
        "\n",
        "      # Set the path to save \n",
        "      path = '/content/Topic-Modeling-Reclame-Aqui/results/dataset/preprocessed/'\n",
        "\n",
        "      # Use makedirs() to create a new directory if it does not exists\n",
        "      if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "      # Save the DataFrame to a CSV file\n",
        "      df.to_csv(path + f'table_{feature}.csv')\n",
        "      print(f'Dataset saved into {path} directory.')\n",
        "  for feature in features:\n",
        "    df = pd.read_csv(f'/content/Topic-Modeling-Reclame-Aqui/results/dataset/preprocessed/table_{feature}.csv')['corpus']\n",
        "    dfs[feature] = df\n",
        "  return dfs"
      ],
      "metadata": {
        "id": "73uvqVnafwvQ"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FRAME_FEATURE_NAME = 'title'\n",
        "\n",
        "dfs = get_data(features=['title', 'documents'])\n",
        "corpus = dfs[DATA_FRAME_FEATURE_NAME]"
      ],
      "metadata": {
        "id": "oo923nmEwgFF",
        "outputId": "bde0dac6-cff8-4b8a-c654-bce04ce348c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before remove duplicates: (12760, 2)\n",
            "Shape after remove duplicates: (10510, 2)\n",
            "Shape before remove duplicates: (12760, 2)\n",
            "Shape after remove duplicates: (10510, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training a BERTopic Model**\n",
        "\n",
        "The BERTopic algorithm has several advantages over other topic modeling algorithms. It is able to handle sparse data, it is scalable to large datasets, and it is able to learn topics that are not well-defined or are overlapping.\n",
        "\n",
        "As our data language is portuguese we will going to set language to multilingual."
      ],
      "metadata": {
        "id": "AWmOFQ3VOD05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new BERTopic model and train it. By default BERTopic use the paraphrase-multilingual-MiniLM-L12-v2 model for multi language documents. For others model check here [BERTopic sentence transformers](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html#sentence-transformers)"
      ],
      "metadata": {
        "id": "_mBsnRnhgbm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new BERTopic model using multilingual option\n",
        "topic_model = BERTopic(language=\"multilingual\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Train model \n",
        "topics, probs = topic_model.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "B0EFqnutN2Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTopic works in three main steps: \n",
        "\n",
        "\n",
        "1.   Documents are first converted to numeric data. It extracts different embeddings based on the context of the word. For this, a sentence transformation model is used.\n",
        "2.  Documents with similar topics are then grouped together forming clusters with similar topics. For this purpose, BERTopic uses the clustering algorithm UMAP to lower the dimensionality of the embeddings. Then the documents are clustered with the density-based algorithm HDBSCAN.\n",
        "3. BERTopic extracts topics from clusters using a class-based TF-IDF score. This score gives the importance of each word in a cluster. Topics are then created based on the most important words measured by their C-TF-IDF score.\n",
        "\n",
        "For more information check this link [BERTopic](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)\n",
        "\n"
      ],
      "metadata": {
        "id": "vnG5aC6ekXeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting Topics**"
      ],
      "metadata": {
        "id": "GimKIMmaRZSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the most frequent topics\n",
        "freq = topic_model.get_topic_info()\n",
        "\n",
        "# Show the top 5 most frequent topics\n",
        "freq.head(5)"
      ],
      "metadata": {
        "id": "JIrBNmF-Rcf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table above shows the five most freqeuente topics and the words present on it extract by BERTopic. -1 refers to all outliers and should be ignored."
      ],
      "metadata": {
        "id": "TVWcfF5mRrMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show the most frequent topic\n",
        "topic_model.get_topic(0)"
      ],
      "metadata": {
        "id": "Br_UMf1kRxU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** BERTopic is stocastich which means that the topics might differ across runs this is mostly due to the stocastisch nature of UMAP"
      ],
      "metadata": {
        "id": "YpP4HgF2R6Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save topic info table as CSV**"
      ],
      "metadata": {
        "id": "CxWLChzJlPmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/topic_info_tables/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "# Save table as csv\n",
        "freq.head(10).to_csv(path + 'topic_info_preprocessed_lemma.csv')"
      ],
      "metadata": {
        "id": "9wM4Ymh_lOdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualization**"
      ],
      "metadata": {
        "id": "wBsA8hiVSJDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Intertopic Distance Map**\n",
        "\n",
        "This graph shows the distance intertopic and help us understand the promixity of topics"
      ],
      "metadata": {
        "id": "2twooZDXUc2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_topics(width=800, height=800)\n",
        "fig"
      ],
      "metadata": {
        "id": "Ae2JSAENSIOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save intertopic distance map**"
      ],
      "metadata": {
        "id": "VEy11A_SlliP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/intertopic_distance_map/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"idm_preprocessed_lemma.png\", format=\"png\")\n",
        "fig.write_html(path + \"idm_preprocessed_lemma.html\")"
      ],
      "metadata": {
        "id": "hLy_Eeu4lqCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/intertopic_distance_map/idm_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "Jf-iwAsQoOCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Topic Hierarchy**\n",
        "\n",
        "The topics that were created can be hierarchically reduced. This visualization shows how the topics relate to one another."
      ],
      "metadata": {
        "id": "r-5bT7AgSgsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_hierarchy(top_n_topics=30, width=800, height=800)\n",
        "fig"
      ],
      "metadata": {
        "id": "TEoOpDD6S4D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save Hierarchical Clustering**"
      ],
      "metadata": {
        "id": "CyM_L5c6pBXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/hierarchical_clustering/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"hc_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "CHYmk_TZpHPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/hierarchical_clustering/hc_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "VrqyRupApBx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Terms**\n",
        "\n",
        "We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation."
      ],
      "metadata": {
        "id": "uC26Cc-6UvYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_barchart(top_n_topics=12, width=300, height=300)\n",
        "fig"
      ],
      "metadata": {
        "id": "-NoMSyeWVF2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save Top Word Scores Bar Chart**"
      ],
      "metadata": {
        "id": "MZWFicVqpWUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/top_words_scores/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"tws_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "Qrz2ywMapdO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/top_words_scores/tws_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "HtFLmwl4pTAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Topic Similarity**\n",
        "\n",
        "This plot shows a similarity matrix by simply applying cosine similarities through those topic embeddings generate by BERTopic through both c-TF-IDF and embeddings. This matrix indicate how similar certain topics are to each other."
      ],
      "metadata": {
        "id": "qMjOvmYVVUDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_heatmap(n_clusters=20, width=800, height=800)\n",
        "fig"
      ],
      "metadata": {
        "id": "_GL5qZ0nV3M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save Similarity Matrix**"
      ],
      "metadata": {
        "id": "CF3QJy6epmQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/similarity_matrix/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"sm_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "VQOAZurPprA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/similarity_matrix/sm_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "y1pa1kWApYgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Term Score Decline**\n",
        "\n",
        "Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added."
      ],
      "metadata": {
        "id": "vd41xsx7Xhgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_term_rank()\n",
        "fig"
      ],
      "metadata": {
        "id": "Rw1wXTEtX967"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save Term score decline per Topic**"
      ],
      "metadata": {
        "id": "6UXg4gpFpwZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/term_score_decline_topic/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"tsdp_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "F5Gppkjpp0hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/term_socore_decline_topic/tsdp_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "QERxP9EspLjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Term search**"
      ],
      "metadata": {
        "id": "AZIV-p12ZMXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find topics that contains blackfriday term\n",
        "similar_topics, similarity = topic_model.find_topics(\"blackfriday\", top_n=5)\n",
        "\n",
        "# Show similar topics\n",
        "similar_topics"
      ],
      "metadata": {
        "id": "pHHayMGRZOob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a specific topic\n",
        "topic_model.get_topic(64)"
      ],
      "metadata": {
        "id": "2BnwvRotZUya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hiperparameter optimization**"
      ],
      "metadata": {
        "id": "f3Q_mQRNPRhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install optuna\n",
        "!pip install hdbscan\n",
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "5vxgob5fPpc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import optuna # for hyperparameter optimization\n",
        "from hdbscan import HDBSCAN # for clustering\n",
        "from umap import UMAP # for dimension reduction\n",
        "from sklearn.feature_extraction.text import CountVectorizer # for convert text documents to matrix of tokens count\n",
        "from bertopic.vectorizers import ClassTfidfTransformer \n",
        "import ast # for convert str to tuple"
      ],
      "metadata": {
        "id": "qQOTWqh-PTwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_documents(model, corpus, label, reduced):\n",
        "  fig = topic_model.visualize_documents(corpus, hide_document_hover=True, hide_annotations=True, width=800, height=800)\n",
        "\n",
        "  # Set the path to save \n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/documents_n_topics/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"document_n_topics_trial_{label}.png\", format=\"png\")\n",
        "  fig.write_html(path + f\"document_n_topics_trial_{label}.html\")"
      ],
      "metadata": {
        "id": "O-l1u6wucN1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_topics(model, label, reduced=False):\n",
        "  fig = model.visualize_topics(width=800, height=800)\n",
        "\n",
        "  # Set the path to save \n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/intertopic_distance_map/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"intertopic_distance_map_trial_{label}.png\", format=\"png\")\n",
        "  fig.write_html(path + f\"intertopic_distance_map_trial_{label}.html\")"
      ],
      "metadata": {
        "id": "zEr6GcxHZptw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_hierarchy(model, label, reduced=False):\n",
        "  fig = model.visualize_hierarchy(width=800, height=800)\n",
        "\n",
        "  # Set the path to save \n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/hierarchical_clustering/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"hierarchical_clustering_trial_{label}.png\", format=\"png\")\n",
        "  fig.write_html(path + f\"hierarchical_clustering_trial_{label}.html\")"
      ],
      "metadata": {
        "id": "9XqbXn0KZ3r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_top_words_scores(model, label, reduced=False):\n",
        "  fig = model.visualize_barchart(top_n_topics=12, width=300, height=300)\n",
        "\n",
        "  # Set the path to save \n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/top_words_scores/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"top_words_scores_trial_{label}.png\", format=\"png\")\n",
        "  fig.write_html(path + f\"top_words_scores_trial_{label}.html\")"
      ],
      "metadata": {
        "id": "A_21t9rmZ4Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_similarity_matrix(model, label, reduced=False):\n",
        "  fig = model.visualize_heatmap()\n",
        "\n",
        "  # Set the path to save \n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/similarity_matrix/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"similarity_matrix_trial_{label}.png\", format=\"png\")\n",
        "  fig.write_html(path + f\"similarity_matrix_trial_{label}.html\")"
      ],
      "metadata": {
        "id": "lTV5l7q7Z45m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_term_rank(model, label, reduced=False):\n",
        "  fig = model.visualize_term_rank(width=800, height=800)\n",
        "\n",
        "  # Set the path to save \n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/term_score_decline_topic/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"term_score_trial_{label}.png\", format=\"png\")\n",
        "  fig.write_html(path + f\"term_score_trial_{label}.html\")"
      ],
      "metadata": {
        "id": "Ag4Yv6r2bfeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_hyperparameters(trial_params, label):\n",
        "  # Set the path to save \n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/hyperparameters/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  with open(path + f\"hyperparameters_trial_{label}.json\", \"w\") as f:\n",
        "    f.write(json.dumps(trial_params))\n"
      ],
      "metadata": {
        "id": "3wuSDGh4dxlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_freq_topics(model, label):\n",
        "   # Print the most frequent topics\n",
        "  freq = topic_model.get_topic_info()\n",
        "\n",
        "  # Show the top 5 most frequent topics\n",
        "  freq = freq.head(5)\n",
        "\n",
        "  # Set the path to save \n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/frequent_topics/' \n",
        "  freq.to_html(path + f\"freq_topics_trial_{label}.json\")\n",
        "  freq.to_csv(path + f\"freq_topics_trial_{label}.csv\")"
      ],
      "metadata": {
        "id": "DQcgSa44ZtdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizer(trial):\n",
        "\n",
        "  # BERTopic hyperparameters\n",
        "  top_n_words = trial.suggest_int('bertopic__top_n_words', 5, 14)\n",
        "  n_gram_range = ast.literal_eval(trial.suggest_categorical('bertopic__n_gram_range', ['(1,1)', '(1,2)', '(1,3)']))\n",
        "  min_topic_size = trial.suggest_int('bertopic__min_topic_size', 15, 20)\n",
        "  diversity = trial.suggest_float('bertopic__diversity', 0.1, 0.9)\n",
        "  outlier_threshold = trial.suggest_float('bertopic__outliers_threshold', 0.04, 0.09)\n",
        "  nr_topics = trial.suggest_int('bertopic__nr_topics', 12, 20)\n",
        "\n",
        "  # UMAP hyperparameters\n",
        "  n_neighbors = trial.suggest_int('umap__n_neighbors', 10, 20)\n",
        "  n_components = trial.suggest_int('umap__n_components', 2, 6)\n",
        "  metric = trial.suggest_categorical('umap__metric', ['cosine', 'euclidean'])\n",
        "  min_dist = trial.suggest_float('umap__min_dist', 0.1, 1.0)\n",
        "  spread = trial.suggest_float('umap__spread', 0.1, 1.0)\n",
        "\n",
        "  # HDBSCAN hyperprarameters\n",
        "  min_cluster_size = trial.suggest_int('hdbscan__min_cluster_size', 10, 20)\n",
        "  min_samples = trial.suggest_int('hdbscan__min_samples', 10, 20)\n",
        "  cluster_selection_epsilon = trial.suggest_float('hdbscan__cluster_selection_epsilon', 0.1, 1.0)\n",
        "\n",
        "  # CountVectorizer hyperparameters \n",
        "  max_features = trial.suggest_int('vectorizer__max_features', 3000, 6000)\n",
        "\n",
        "  # reduce the impact of frequent words.\n",
        "  ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "\n",
        "  # list of portuguese stop words + custom words\n",
        "  stop_words = nltk.corpus.stopwords.words('portuguese') + custom_stop_words\n",
        "\n",
        "  # create a new CountVectorizer to create a matrix of tokens count\n",
        "  vectorizer_model = CountVectorizer(stop_words=stop_words, ngram_range=n_gram_range, max_features=max_features)\n",
        "\n",
        "  # create a new UMAP model to reduce dimension\n",
        "  umap_model = UMAP(n_neighbors=n_neighbors, metric=metric, random_state=42)\n",
        "\n",
        "  # create a new HDBSCAN model to cluster documents\n",
        "  hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
        "\n",
        "  # create a new BERTopic model using multilingual option\n",
        "  model = BERTopic(language=\"multilingual\", \n",
        "                         calculate_probabilities=True, \n",
        "                         verbose=False,\n",
        "                         nr_topics='auto',\n",
        "                         top_n_words=top_n_words,\n",
        "                         n_gram_range=n_gram_range,\n",
        "                         min_topic_size=min_topic_size,\n",
        "                         diversity=diversity,\n",
        "                         umap_model=umap_model,\n",
        "                         hdbscan_model=hdbscan_model,\n",
        "                         vectorizer_model=vectorizer_model,\n",
        "                         ctfidf_model=ctfidf_model)\n",
        "  \n",
        "  label = trial.number\n",
        "  params = trial.params\n",
        "\n",
        "  # train BERTopic model \n",
        "  topics, probs = model.fit_transform(corpus)\n",
        "\n",
        "  # save plots\n",
        "  save_topics(model, label, False)\n",
        "  save_documents(model, corpus, label, False)\n",
        "  save_hierarchy(model, label, False)\n",
        "  ##save_similarity_matrix(model, label, False)\n",
        "  save_term_rank(model, label, False)\n",
        "  save_top_words_scores(model, label, False)\n",
        "\n",
        "  # reduce outliers\n",
        "  new_topics = model.reduce_outliers(corpus,\n",
        "                                     topics, \n",
        "                                     probabilities=probs,\n",
        "                                     strategy=\"probabilities\",\n",
        "                                     threshold=outlier_threshold)\n",
        "  \n",
        "  # update model topics after outlier reduction\n",
        "  model.update_topics(corpus, topics=new_topics)\n",
        "\n",
        "  # save plots after reducion\n",
        "  save_topics(model, label, True)\n",
        "  save_documents(model, corpus, label, True)\n",
        "  save_hierarchy(model, label, True)\n",
        "  ##save_similarity_matrix(model, label, True)\n",
        "  save_term_rank(model, label, True)\n",
        "  save_top_words_scores(model, label, True)\n",
        "\n",
        "  # save hyperparameters\n",
        "  save_hyperparameters(params, label)\n",
        "\n",
        "  # save reduction result\n",
        "  save_freq_topics(model, label)\n",
        "\n",
        "  # save model\n",
        "  path = f'/content/Topic-Modeling-Reclame-Aqui/results/{DATA_FRAME_FEATURE_NAME}/models/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  model.save(path + f\"model_trial_{label}\")\n",
        "\n",
        "  return 0"
      ],
      "metadata": {
        "id": "O87BZ5kyQEvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# define the number of models to generate by optuna\n",
        "NUMBER_OF_MODELS = 15\n",
        "\n",
        "# create a new study\n",
        "study = optuna.create_study(study_name='Bertopic')\n",
        "\n",
        "# run the optmize function \n",
        "study.optimize(optimizer, n_trials=NUMBER_OF_MODELS)"
      ],
      "metadata": {
        "id": "57mnK6MfYAp4",
        "outputId": "51275c23-262a-4173-84cc-3a2abef618c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-01-06 19:21:56,054]\u001b[0m A new study created in memory with name: Bertopic\u001b[0m\n",
            "2023-01-06 19:22:15,230 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:22:35,132 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:22:37,271 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:22:42,540 - BERTopic - Reduced number of topics from 14 to 14\n",
            "\u001b[32m[I 2023-01-06 19:23:54,203]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 5, 'bertopic__n_gram_range': '(1,1)', 'bertopic__min_topic_size': 15, 'bertopic__diversity': 0.746830833426787, 'bertopic__outliers_threshold': 0.06411398360593415, 'bertopic__nr_topics': 14, 'umap__n_neighbors': 15, 'umap__n_components': 6, 'umap__metric': 'euclidean', 'umap__min_dist': 0.25133308240297303, 'umap__spread': 0.11373918912306409, 'hdbscan__min_cluster_size': 20, 'hdbscan__min_samples': 12, 'hdbscan__cluster_selection_epsilon': 0.11754072133120577, 'vectorizer__max_features': 4868}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:24:12,647 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:24:26,393 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:24:27,565 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:24:34,531 - BERTopic - Reduced number of topics from 21 to 21\n",
            "\u001b[32m[I 2023-01-06 19:25:54,704]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 13, 'bertopic__n_gram_range': '(1,2)', 'bertopic__min_topic_size': 20, 'bertopic__diversity': 0.8461359798486624, 'bertopic__outliers_threshold': 0.0865088293053683, 'bertopic__nr_topics': 14, 'umap__n_neighbors': 12, 'umap__n_components': 6, 'umap__metric': 'euclidean', 'umap__min_dist': 0.9442704986949149, 'umap__spread': 0.5350744839069305, 'hdbscan__min_cluster_size': 17, 'hdbscan__min_samples': 10, 'hdbscan__cluster_selection_epsilon': 0.7394533737440608, 'vectorizer__max_features': 5070}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:26:11,321 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:26:26,846 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:26:27,892 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:26:35,258 - BERTopic - Reduced number of topics from 12 to 12\n",
            "\u001b[32m[I 2023-01-06 19:27:53,750]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 11, 'bertopic__n_gram_range': '(1,2)', 'bertopic__min_topic_size': 18, 'bertopic__diversity': 0.48061455897361904, 'bertopic__outliers_threshold': 0.06115453337124922, 'bertopic__nr_topics': 20, 'umap__n_neighbors': 17, 'umap__n_components': 6, 'umap__metric': 'euclidean', 'umap__min_dist': 0.8104606008338715, 'umap__spread': 0.5791186476370384, 'hdbscan__min_cluster_size': 13, 'hdbscan__min_samples': 13, 'hdbscan__cluster_selection_epsilon': 0.4051198986302088, 'vectorizer__max_features': 5979}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:28:12,004 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:28:26,958 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:28:28,794 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:28:32,830 - BERTopic - Reduced number of topics from 29 to 23\n",
            "\u001b[32m[I 2023-01-06 19:29:45,653]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 10, 'bertopic__n_gram_range': '(1,1)', 'bertopic__min_topic_size': 19, 'bertopic__diversity': 0.14973737535537676, 'bertopic__outliers_threshold': 0.04469057823633308, 'bertopic__nr_topics': 17, 'umap__n_neighbors': 15, 'umap__n_components': 2, 'umap__metric': 'cosine', 'umap__min_dist': 0.8775868496883112, 'umap__spread': 0.5620923855080658, 'hdbscan__min_cluster_size': 15, 'hdbscan__min_samples': 20, 'hdbscan__cluster_selection_epsilon': 0.5418988813510622, 'vectorizer__max_features': 4245}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:30:02,003 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:30:15,427 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:30:21,831 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:30:35,333 - BERTopic - Reduced number of topics from 95 to 18\n",
            "\u001b[32m[I 2023-01-06 19:31:43,321]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 14, 'bertopic__n_gram_range': '(1,1)', 'bertopic__min_topic_size': 17, 'bertopic__diversity': 0.14367605397034558, 'bertopic__outliers_threshold': 0.07475876099620976, 'bertopic__nr_topics': 16, 'umap__n_neighbors': 13, 'umap__n_components': 2, 'umap__metric': 'cosine', 'umap__min_dist': 0.5182463000694892, 'umap__spread': 0.39697972611882726, 'hdbscan__min_cluster_size': 12, 'hdbscan__min_samples': 16, 'hdbscan__cluster_selection_epsilon': 0.49500907621478596, 'vectorizer__max_features': 3901}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:32:00,430 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:32:18,893 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:32:21,085 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:32:24,368 - BERTopic - Reduced number of topics from 27 to 10\n",
            "\u001b[32m[I 2023-01-06 19:33:34,213]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 7, 'bertopic__n_gram_range': '(1,1)', 'bertopic__min_topic_size': 18, 'bertopic__diversity': 0.6252879206247676, 'bertopic__outliers_threshold': 0.0425297249126716, 'bertopic__nr_topics': 18, 'umap__n_neighbors': 12, 'umap__n_components': 6, 'umap__metric': 'euclidean', 'umap__min_dist': 0.8551420700704142, 'umap__spread': 0.13889099668706387, 'hdbscan__min_cluster_size': 11, 'hdbscan__min_samples': 17, 'hdbscan__cluster_selection_epsilon': 0.5277780436176482, 'vectorizer__max_features': 3873}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:33:50,548 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:34:04,575 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:34:05,626 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:34:08,643 - BERTopic - Reduced number of topics from 15 to 15\n",
            "\u001b[32m[I 2023-01-06 19:35:18,265]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 10, 'bertopic__n_gram_range': '(1,1)', 'bertopic__min_topic_size': 16, 'bertopic__diversity': 0.6998729377561357, 'bertopic__outliers_threshold': 0.07467814208460849, 'bertopic__nr_topics': 20, 'umap__n_neighbors': 13, 'umap__n_components': 2, 'umap__metric': 'euclidean', 'umap__min_dist': 0.9669716146366326, 'umap__spread': 0.7151566044476736, 'hdbscan__min_cluster_size': 15, 'hdbscan__min_samples': 10, 'hdbscan__cluster_selection_epsilon': 0.7104627478689681, 'vectorizer__max_features': 4162}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:35:34,631 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:35:52,625 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:35:54,464 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:36:02,638 - BERTopic - Reduced number of topics from 12 to 12\n",
            "\u001b[32m[I 2023-01-06 19:37:12,099]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 6, 'bertopic__n_gram_range': '(1,2)', 'bertopic__min_topic_size': 18, 'bertopic__diversity': 0.24192062721174976, 'bertopic__outliers_threshold': 0.07203680574356533, 'bertopic__nr_topics': 16, 'umap__n_neighbors': 14, 'umap__n_components': 6, 'umap__metric': 'euclidean', 'umap__min_dist': 0.8906461416772852, 'umap__spread': 0.23616411329203982, 'hdbscan__min_cluster_size': 12, 'hdbscan__min_samples': 19, 'hdbscan__cluster_selection_epsilon': 0.674992524087891, 'vectorizer__max_features': 5368}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:37:29,695 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:37:48,290 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:37:49,263 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:37:55,547 - BERTopic - Reduced number of topics from 12 to 12\n",
            "\u001b[32m[I 2023-01-06 19:39:14,762]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 13, 'bertopic__n_gram_range': '(1,2)', 'bertopic__min_topic_size': 20, 'bertopic__diversity': 0.565311494042093, 'bertopic__outliers_threshold': 0.08265272339814092, 'bertopic__nr_topics': 19, 'umap__n_neighbors': 17, 'umap__n_components': 5, 'umap__metric': 'euclidean', 'umap__min_dist': 0.15992691418141897, 'umap__spread': 0.3842734321552167, 'hdbscan__min_cluster_size': 18, 'hdbscan__min_samples': 11, 'hdbscan__cluster_selection_epsilon': 0.13705137113887828, 'vectorizer__max_features': 3412}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:39:32,288 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:39:44,311 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:39:46,195 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:39:50,660 - BERTopic - Reduced number of topics from 30 to 30\n",
            "\u001b[32m[I 2023-01-06 19:41:03,580]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 13, 'bertopic__n_gram_range': '(1,1)', 'bertopic__min_topic_size': 17, 'bertopic__diversity': 0.791618313312353, 'bertopic__outliers_threshold': 0.06122546757647052, 'bertopic__nr_topics': 13, 'umap__n_neighbors': 10, 'umap__n_components': 6, 'umap__metric': 'cosine', 'umap__min_dist': 0.19818994233930257, 'umap__spread': 0.6121359067793032, 'hdbscan__min_cluster_size': 15, 'hdbscan__min_samples': 14, 'hdbscan__cluster_selection_epsilon': 0.42671842284846306, 'vectorizer__max_features': 5708}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:41:22,471 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:41:42,757 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:41:43,786 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:41:55,704 - BERTopic - Reduced number of topics from 16 to 16\n",
            "\u001b[32m[I 2023-01-06 19:43:16,704]\u001b[0m Trial 10 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 5, 'bertopic__n_gram_range': '(1,3)', 'bertopic__min_topic_size': 15, 'bertopic__diversity': 0.4235442543162875, 'bertopic__outliers_threshold': 0.053634957587181654, 'bertopic__nr_topics': 12, 'umap__n_neighbors': 20, 'umap__n_components': 4, 'umap__metric': 'euclidean', 'umap__min_dist': 0.39257899009870706, 'umap__spread': 0.9968461090624116, 'hdbscan__min_cluster_size': 20, 'hdbscan__min_samples': 12, 'hdbscan__cluster_selection_epsilon': 0.9810555407139644, 'vectorizer__max_features': 4695}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:43:35,873 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:43:57,166 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:43:57,946 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:44:11,197 - BERTopic - Reduced number of topics from 7 to 7\n",
            "\u001b[32m[I 2023-01-06 19:45:47,879]\u001b[0m Trial 11 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 8, 'bertopic__n_gram_range': '(1,3)', 'bertopic__min_topic_size': 15, 'bertopic__diversity': 0.8994004054876489, 'bertopic__outliers_threshold': 0.0898033320156379, 'bertopic__nr_topics': 14, 'umap__n_neighbors': 10, 'umap__n_components': 4, 'umap__metric': 'euclidean', 'umap__min_dist': 0.6580647838541949, 'umap__spread': 0.8124600795115974, 'hdbscan__min_cluster_size': 18, 'hdbscan__min_samples': 10, 'hdbscan__cluster_selection_epsilon': 0.10811331542848923, 'vectorizer__max_features': 4947}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:46:05,426 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:46:29,024 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:46:30,525 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:46:38,268 - BERTopic - Reduced number of topics from 14 to 14\n",
            "\u001b[32m[I 2023-01-06 19:48:05,725]\u001b[0m Trial 12 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 8, 'bertopic__n_gram_range': '(1,2)', 'bertopic__min_topic_size': 20, 'bertopic__diversity': 0.8904091149260074, 'bertopic__outliers_threshold': 0.06753071363237483, 'bertopic__nr_topics': 14, 'umap__n_neighbors': 17, 'umap__n_components': 5, 'umap__metric': 'euclidean', 'umap__min_dist': 0.3391649025110841, 'umap__spread': 0.33447523430374376, 'hdbscan__min_cluster_size': 20, 'hdbscan__min_samples': 12, 'hdbscan__cluster_selection_epsilon': 0.8875578312746322, 'vectorizer__max_features': 5138}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:48:27,441 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:48:42,713 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:48:44,278 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:48:59,558 - BERTopic - Reduced number of topics from 21 to 21\n",
            "\u001b[32m[I 2023-01-06 19:50:23,394]\u001b[0m Trial 13 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 12, 'bertopic__n_gram_range': '(1,2)', 'bertopic__min_topic_size': 16, 'bertopic__diversity': 0.7383338418998517, 'bertopic__outliers_threshold': 0.05553891749718943, 'bertopic__nr_topics': 15, 'umap__n_neighbors': 12, 'umap__n_components': 5, 'umap__metric': 'euclidean', 'umap__min_dist': 0.6423990581782952, 'umap__spread': 0.13969225882941413, 'hdbscan__min_cluster_size': 18, 'hdbscan__min_samples': 14, 'hdbscan__cluster_selection_epsilon': 0.27296821200567173, 'vectorizer__max_features': 4785}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "2023-01-06 19:50:41,437 - BERTopic - Transformed documents to Embeddings\n",
            "2023-01-06 19:51:00,868 - BERTopic - Reduced dimensionality\n",
            "2023-01-06 19:51:01,706 - BERTopic - Clustered reduced embeddings\n",
            "2023-01-06 19:51:12,953 - BERTopic - Reduced number of topics from 9 to 9\n",
            "\u001b[32m[I 2023-01-06 19:52:43,140]\u001b[0m Trial 14 finished with value: 0.0 and parameters: {'bertopic__top_n_words': 5, 'bertopic__n_gram_range': '(1,3)', 'bertopic__min_topic_size': 19, 'bertopic__diversity': 0.8069165061725976, 'bertopic__outliers_threshold': 0.08154603902387678, 'bertopic__nr_topics': 12, 'umap__n_neighbors': 16, 'umap__n_components': 3, 'umap__metric': 'euclidean', 'umap__min_dist': 0.332983831584028, 'umap__spread': 0.8355199588560615, 'hdbscan__min_cluster_size': 17, 'hdbscan__min_samples': 10, 'hdbscan__cluster_selection_epsilon': 0.7337192479571087, 'vectorizer__max_features': 5405}. Best is trial 0 with value: 0.0.\u001b[0m\n",
            "\u001b[33m[W 2023-01-06 19:52:47,656]\u001b[0m Trial 15 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-85-244b2aaea613>\", line 59, in optimizer\n",
            "    topics, probs = model.fit_transform(corpus)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/bertopic/_bertopic.py\", line 339, in fit_transform\n",
            "    embeddings = self._extract_embeddings(documents.Document,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/bertopic/_bertopic.py\", line 2787, in _extract_embeddings\n",
            "    embeddings = self.embedding_model.embed_documents(documents, verbose)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/bertopic/backend/_base.py\", line 69, in embed_documents\n",
            "    return self.embed(document, verbose)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/bertopic/backend/_sentencetransformers.py\", line 63, in embed\n",
            "    embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sentence_transformers/SentenceTransformer.py\", line 188, in encode\n",
            "    embeddings = embeddings.cpu()\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \"\"\"\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    420\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     ):\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-244b2aaea613>\u001b[0m in \u001b[0;36moptimizer\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;31m# train BERTopic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;31m# save plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[1;32m    337\u001b[0m             self.embedding_model = select_backend(self.embedding_model,\n\u001b[1;32m    338\u001b[0m                                                   language=self.language)\n\u001b[0;32m--> 339\u001b[0;31m             embeddings = self._extract_embeddings(documents.Document,\n\u001b[0m\u001b[1;32m    340\u001b[0m                                                   \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                                   verbose=self.verbose)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_extract_embeddings\u001b[0;34m(self, documents, method, verbose)\u001b[0m\n\u001b[1;32m   2785\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"document\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             raise ValueError(\"Wrong method for extracting document/word embeddings. \"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bertopic/backend/_base.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, document, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bertopic/backend/_sentencetransformers.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, documents, verbose)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0;31m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mall_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Merge of similar topics**"
      ],
      "metadata": {
        "id": "kmSbzWH0gJHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: select the best model and merge similar topic and save final model\n",
        "\n",
        "#topic_model = BERTopic.load(\"best_model\")"
      ],
      "metadata": {
        "id": "OWI9jDFYgTLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########33!rm -rf /content/Topic-Modeling-Reclame-Aqui/results"
      ],
      "metadata": {
        "id": "6f8Adw95AVkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ssh-keygen -t rsa -b 4096\n",
        "# Add github.com to our known hosts\n",
        "! ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "# Restrict the key permissions, or else SSH will complain.\n",
        "! chmod go-rwx /root/.ssh/id_rsa"
      ],
      "metadata": {
        "id": "yVd1eoa0zK7f",
        "outputId": "eda0dd0f-1a5f-4033-eb25-bba93bd7924d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_rsa): \n",
            "/root/.ssh/id_rsa already exists.\n",
            "Overwrite (y/n)? Y\n",
            "Enter passphrase (empty for no passphrase): \n",
            "Enter same passphrase again: \n",
            "Your identification has been saved in /root/.ssh/id_rsa.\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub.\n",
            "The key fingerprint is:\n",
            "SHA256:8+eNknI/3SRVFqCNFQI1Sx4LQRmHyWsxaWXIjLBgnR8 root@66134f0b36cb\n",
            "The key's randomart image is:\n",
            "+---[RSA 4096]----+\n",
            "|   o..o **%X +o..|\n",
            "|  . .o.E &* @   o|\n",
            "|     .. o +* . ..|\n",
            "|       . o      .|\n",
            "|        S      . |\n",
            "|         o    . .|\n",
            "|          .... + |\n",
            "|        . +o.o. .|\n",
            "|         o o+..  |\n",
            "+----[SHA256]-----+\n",
            "# github.com:22 SSH-2.0-babeld-4ce3b487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "id": "_z-PSHRzz0PY",
        "outputId": "e13bb293-cef5-4681-bd19-0a8fb1543bd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDEQ/953oQTI25HaIZyMi5vtWXXP0qy9R4lq10XSJrgU9SW+6gEML3a5Wfl8wfeKxMBTMo0ICJC86ermSQWhMg6pyeZvRin9R8MwVJxyUfbFCMkwmMnSV9JxtJpg6TpXp8qzJ304r+9SyauYjo6QcDBXAC1H7KhSCTiSAZWJuuf5+xvvzPFbZ4Zp4kyFOKzvDiFnTiTRs6WRKW9liQ7kguDOK67PUBmTGsgGweAZOPH2jI86Jkg8G4m2565fgGOznVUuPVxRoL0KCcflnOGykSCJez7gRUMvqCy6uy9cTkwHSTDSvvx/CxYQ3AOkioqQ+x6ehCrGRxw+ICKhtWuhblGQKLlHXcgKUda7cvQo4mPb3Y19WdtXwdUD5Gq/GWRcxBBs3Qg8SaVHEboeFQaehPIumenGf+xQ1LB39oGBkbDOWKSZnOs9PA0lro7qbCLzNQkjmcmM9R9rDxv2GaeX2Jo1uqSYiNRFveP2rPbQEPVvXBEMDo84FEkmQ84msEvOUENRSlJyOCQlCh+a3u71JgT8jXUhqFH9FukTHRyygQk8UggdlnW/BSqaGRuzjr6UVjBN+wiGMmwvSLFwPUOdCDC3Vy1HuMXcjR1vZe6i4ogMtR0+vYTJv4dZOwNg5+ZiJR4eVCQCZB7sc917gwiB3ytd7ISjaviQgbBp6HfhP+jqQ== root@66134f0b36cb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"mattheus_ribeiro@outlook.com\"\n",
        "!git config --global user.name \"punkmic\""
      ],
      "metadata": {
        "id": "4iixpGcjzxrO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh -T git@github.com"
      ],
      "metadata": {
        "id": "bcqhgkdaz3QI",
        "outputId": "8c089867-5e6f-462a-934b-dbf9509bcf76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added the RSA host key for IP address '140.82.114.4' to the list of known hosts.\r\n",
            "Hi punkmic! You've successfully authenticated, but GitHub does not provide shell access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone git@github.com:punkmic/Topic-Modeling-Reclame-Aqui.git"
      ],
      "metadata": {
        "id": "zSA4ju5NDpqK",
        "outputId": "17af36b1-3642-470e-ecb5-26d32c48b5f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Topic-Modeling-Reclame-Aqui'...\n",
            "Warning: Permanently added the RSA host key for IP address '20.201.28.151' to the list of known hosts.\n",
            "remote: Enumerating objects: 16267, done.\u001b[K\n",
            "remote: Counting objects: 100% (2086/2086), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1818/1818), done.\u001b[K\n",
            "remote: Total 16267 (delta 293), reused 1951 (delta 234), pack-reused 14181\u001b[K\n",
            "Receiving objects: 100% (16267/16267), 126.63 MiB | 7.41 MiB/s, done.\n",
            "Resolving deltas: 100% (1441/1441), done.\n",
            "Checking out files: 100% (14808/14808), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Topic-Modeling-Reclame-Aqui/"
      ],
      "metadata": {
        "id": "2eEK0j1L6b7p",
        "outputId": "f1e61a6c-a1e6-4a45-b2f5-c97c94524f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Topic-Modeling-Reclame-Aqui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ./results/dataset/preprocessed/table.csv"
      ],
      "metadata": {
        "id": "CGqBhuS6z7WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "eyMFQF889jtB",
        "outputId": "22b5499f-0c98-4735-8f33-001582cdeab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mnew file:   results/dataset/preprocessed/table.csv\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Adding preprocessed dataset\""
      ],
      "metadata": {
        "id": "PFY8B1Ikz-U-",
        "outputId": "2de09364-7da9-4f51-e26a-6cb20517eb5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master 30c507c] Adding preprocessed dataset\n",
            " 1 file changed, 10511 insertions(+)\n",
            " create mode 100644 results/dataset/preprocessed/table.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin master"
      ],
      "metadata": {
        "id": "uEJWirD70EiH",
        "outputId": "3b9ffb12-c479-47d9-9f06-9de85536e408",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting objects: 6, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (6/6), 8.25 MiB | 1.45 MiB/s, done.\n",
            "Total 6 (delta 2), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "remote: warning: See http://git.io/iEPt8g for more information.\u001b[K\n",
            "remote: warning: File results/dataset/preprocessed/table.csv is 68.94 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: warning: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To github.com:punkmic/Topic-Modeling-Reclame-Aqui.git\n",
            "   c9fb74b..30c507c  master -> master\n"
          ]
        }
      ]
    }
  ]
}