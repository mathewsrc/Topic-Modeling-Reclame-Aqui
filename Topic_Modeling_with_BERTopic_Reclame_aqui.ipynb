{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCGWx+aRsh2RisdtYyOVhZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/Topic_Modeling_with_BERTopic_Reclame_aqui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modeling with BERTopic - Reclame Aqui**\n",
        "\n",
        "BERTopic is a topic modeling technique that leverages ðŸ¤— transformers and a custom class-based TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions."
      ],
      "metadata": {
        "id": "VG5Z-hvZ9GIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import dependecies**"
      ],
      "metadata": {
        "id": "eXD8VBix7DD1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ0hDdDm6gtj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # for read csv file and manipulate data\n",
        "import csv\n",
        "import os\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "try:\n",
        "  from bertopic import BERTopic # for topic modeling\n",
        "  from enelvo.normaliser import Normaliser # for spelling errors and abbreviations\n",
        "except:\n",
        "  !pip install enelvo --no-dependecies numpy\n",
        "  !pip install bertopic\n",
        "  os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "BvRCi7Cvp0G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "JAvaPg5ep3Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "97RrmPshvcmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "Bo7_jZ5Zvirl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prepare data**\n",
        "\n",
        "The following steps will be done:\n",
        "\n",
        "1- clone the repository from Github\n",
        "\n",
        "2- load our csv file as DataFrame object\n",
        "\n",
        "3- remove duplicate rows\n",
        "\n",
        "4- join title and body columns"
      ],
      "metadata": {
        "id": "EGjKdT4C8RVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/punkmic/Topic-Modeling-Reclame-Aqui.git"
      ],
      "metadata": {
        "id": "C_yss1JC-8Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to update files from remote repository\n",
        "%cd /content/Topic-Modeling-Reclame-Aqui \n",
        "!git pull \n",
        "%cd ..\n",
        "!pwd"
      ],
      "metadata": {
        "id": "uv3QBnDRHFPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load csv \n",
        "path_csv = \"/content/Topic-Modeling-Reclame-Aqui/corpus.csv\"\n",
        "df = pd.read_csv(path_csv)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "F3IoOKARBwVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicates\n",
        "print(f\"Shape before remove duplicates: {df.shape}\")\n",
        "df = df.drop_duplicates(subset=\"text\")\n",
        "print(f\"Shape after remove duplicates: {df.shape}\")"
      ],
      "metadata": {
        "id": "JFru444PE89C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"documents\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "N0iqgleUNVlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.documents.dropna(inplace=True)\n",
        "df.reset_index(inplace = True, drop = True)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "D2OKpjiYO6n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "drjuwA5yk4YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**\n",
        "\n",
        "Tokenization aims to breaking text down into its component parts"
      ],
      "metadata": {
        "id": "i9ukWCy8qBOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_TOKENIZER = nltk.tokenize.word_tokenize\n",
        "\n",
        "def tokenize(text, lowercase=True):\n",
        "  if lowercase:\n",
        "    text = text.lower()\n",
        "  return WORD_TOKENIZER(text, language=\"portuguese\")"
      ],
      "metadata": {
        "id": "k_f8hsxJk7v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stem** \n",
        "\n",
        "Stem the tokens. This step aims to remove morphological affixes and normalize to standardized stem forms"
      ],
      "metadata": {
        "id": "Lm7V--yMq98r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEMMER = nltk.PorterStemmer()\n",
        "\n",
        "def stem(tokens):\n",
        "  return [STEMMER.stem(token) for token in tokens]"
      ],
      "metadata": {
        "id": "TphgFQIxrPMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatize**\n",
        "\n",
        "Lemmatize the tokens. Retains more natural forms than stemming, but assumes all tokens nons unless tokens are passed as (word, pos) tuples."
      ],
      "metadata": {
        "id": "RTiUcqzrrdiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEMMATIZER = nltk.WordNetLemmatizer()\n",
        "def lemmatize(tokens):\n",
        "  lemmas = []\n",
        "  for token in tokens:\n",
        "    if isinstance(token, str):\n",
        "      lemmas.append(LEMMATIZER.lemmatize(token)) # for str token\n",
        "    else:\n",
        "      lemmas.append(LEMMATIZER.lemmatize(*tokens)) # for tuple\n",
        "  return lemmas"
      ],
      "metadata": {
        "id": "4O48ihphrxy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove stopwords**\n",
        "\n",
        "Stop words are things like articles and conjunctions that usually do not offer a lot of value in an analysis."
      ],
      "metadata": {
        "id": "jkLYqJgDs4Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(tokens, stopwords=None):\n",
        "  if stopwords is None:\n",
        "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "  return [token for token in tokens if token not in stopwords]"
      ],
      "metadata": {
        "id": "D9blZaBYtHgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove hyperlinks**\n",
        "\n",
        "Removes http/s links from the tokens."
      ],
      "metadata": {
        "id": "X4qMKyfKtaAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_links(tokens):\n",
        "  return [token for token in tokens \n",
        "          if not token.startswith(\"http://\")\n",
        "          and not token.startswith(\"https://\")]"
      ],
      "metadata": {
        "id": "LqlMta5qtovS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove numbers**"
      ],
      "metadata": {
        "id": "EwKFluprz-pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(tokens):\n",
        "  return [token for token in tokens if not token.isdigit()]"
      ],
      "metadata": {
        "id": "ALKnXv8C0EWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove words with numbers\n",
        "\n",
        "Remove words that contains numbers such as post code"
      ],
      "metadata": {
        "id": "qt7mxZmnLVec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_words_numbers(tokens):\n",
        "  return [token for token in tokens if not re.sub(r'\\w*\\d\\w*', '', token)]\n"
      ],
      "metadata": {
        "id": "cOyU50VaLcQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove punctuation**"
      ],
      "metadata": {
        "id": "7y1FqCD1t75_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(tokens,\n",
        "                       strip_mentions=False,\n",
        "                       strip_hashtags=False,\n",
        "                       strict=False):\n",
        "\n",
        "    tokens = [re.sub(r'(\\W)(?=\\1)', '', t) for t in tokens ] # remove double punctuation, I. e, ..\n",
        "    tokens = [t for t in tokens if t not in string.punctuation]\n",
        "    if strip_mentions:\n",
        "        tokens = [t.lstrip('@') for t in tokens]\n",
        "    if strip_hashtags:\n",
        "        tokens = [t.lstrip('#') for t in tokens]\n",
        "    if strict:\n",
        "        cleaned = []\n",
        "        for t in tokens:\n",
        "            cleaned.append(\n",
        "                t.translate(str.maketrans('', '', string.punctuation)).strip())\n",
        "        tokens = [t for t in cleaned if t]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "ba7paG7_uBtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove tokens with length less than 4 characters**"
      ],
      "metadata": {
        "id": "SiUx2T8C5YBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_tokens(tokens):\n",
        "  return [token for token in tokens if not len(token) <= 4]"
      ],
      "metadata": {
        "id": "t8SOwNYW6QI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Correction of spelling errors and abbreviations**"
      ],
      "metadata": {
        "id": "WqtOisN38pZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "norm = Normaliser(tokenizer='readable')\n",
        "def normalize(text):\n",
        "  return norm.normalise(text)"
      ],
      "metadata": {
        "id": "fUTiJYc_8hba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i, text in enumerate(df.documents):\n",
        "  if i % 1000 == 0:\n",
        "    print(f\"Processed {i} documents\")\n",
        "  tokens = normalize(text)\n",
        "  tokens = tokenize(text)\n",
        "  tokens = remove_links(tokens)\n",
        "  tokens = remove_numbers(tokens)\n",
        "  tokens = remove_words_numbers(tokens)\n",
        "  tokens = remove_short_tokens(tokens)\n",
        "  tokens = remove_stopwords(tokens)\n",
        "  tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n",
        "  tokens = lemmatize(tokens) \n",
        "  corpus.append(' '.join(tokens))"
      ],
      "metadata": {
        "id": "5aiFtbo3uuh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.documents[20])\n",
        "print()\n",
        "corpus[20]"
      ],
      "metadata": {
        "id": "Tu75mPgywHwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training a BERTopic Model**\n",
        "\n",
        "As our data language is portuguese we will going to set language to multilingual."
      ],
      "metadata": {
        "id": "AWmOFQ3VOD05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enabling the GPU**\n",
        "\n",
        "We will use the GPU provided by COLAB to accelarate our model training. To enable GPUs for the notebook:\n",
        "1- Navigate to Edit -> Notebook Settings\n",
        "2- Select GPU from the Hardware Accelerator drop-down"
      ],
      "metadata": {
        "id": "5goOGgeyhK3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify if GPU is selected\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "tgfQItxxRCfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(language=\"multilingual\", calculate_probabilities=True, verbose=True)\n",
        "topics, probs = topic_model.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "B0EFqnutN2Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extracting Topics**"
      ],
      "metadata": {
        "id": "GimKIMmaRZSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the most frequent topics\n",
        "\n",
        "freq = topic_model.get_topic_info()\n",
        "freq.head(5)"
      ],
      "metadata": {
        "id": "JIrBNmF-Rcf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-1 refers to all outliers and should be ignored."
      ],
      "metadata": {
        "id": "TVWcfF5mRrMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show the most frequent topic\n",
        "topic_model.get_topic(1)"
      ],
      "metadata": {
        "id": "Br_UMf1kRxU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** BERTopic is stocastich which means that the topics might differ across runs this is mostly due to the stocastisch nature of UMAP"
      ],
      "metadata": {
        "id": "YpP4HgF2R6Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualization**"
      ],
      "metadata": {
        "id": "wBsA8hiVSJDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Intertopic Distance Map**\n",
        "\n",
        "This graph shows the distance intertopic and help us understand the promixity of topics"
      ],
      "metadata": {
        "id": "2twooZDXUc2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics()"
      ],
      "metadata": {
        "id": "Ae2JSAENSIOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Topic Hierarchy**\n",
        "\n",
        "The topics that were created can be hierarchically reduced. This visualization shows how the topics relate to one another."
      ],
      "metadata": {
        "id": "r-5bT7AgSgsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_hierarchy(top_n_topics=50)"
      ],
      "metadata": {
        "id": "TEoOpDD6S4D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Terms**\n",
        "\n",
        "We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation."
      ],
      "metadata": {
        "id": "uC26Cc-6UvYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart(top_n_topics=5)"
      ],
      "metadata": {
        "id": "-NoMSyeWVF2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Topic SImilarity**\n",
        "\n",
        "This plot shows a similarity matrix by simply applying cosine similarities through those topic embeddings generate by BERTopic through both c-TF-IDF and embeddings. This matrix indicate how similar certain topics are to each other."
      ],
      "metadata": {
        "id": "qMjOvmYVVUDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_heatmap(n_clusters=20, width=1000, height=1000)"
      ],
      "metadata": {
        "id": "_GL5qZ0nV3M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Term Score Decline\n",
        "\n",
        "Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added."
      ],
      "metadata": {
        "id": "vd41xsx7Xhgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_term_rank()"
      ],
      "metadata": {
        "id": "Rw1wXTEtX967"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Term search**"
      ],
      "metadata": {
        "id": "AZIV-p12ZMXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_topics, similarity = topic_model.find_topics(\"blackfriday\", top_n=3)\n",
        "similar_topics"
      ],
      "metadata": {
        "id": "pHHayMGRZOob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(58)"
      ],
      "metadata": {
        "id": "2BnwvRotZUya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Topic Reduction**"
      ],
      "metadata": {
        "id": "Lf78SHIeYzMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new_topic_model = topic_model.reduce_topics(df.documents, nr_topics=40)\n",
        "#new_topics = new_topic_model.topics_\n",
        "#new_probs = new_topic_model.probabilities_"
      ],
      "metadata": {
        "id": "wtVMrXHGY4GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new_topic_model.visualize_topics()"
      ],
      "metadata": {
        "id": "W1mW2iTeaU04"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}