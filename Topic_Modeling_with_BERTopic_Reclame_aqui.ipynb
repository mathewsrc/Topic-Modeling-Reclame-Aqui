{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/Topic_Modeling_with_BERTopic_Reclame_aqui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modeling with BERTopic - Reclame Aqui**\n",
        "\n",
        "BERTopic is a topic modeling technique that leverages transformers and a custom class-based TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions \n",
        "\n",
        "Reference: (https://maartengr.github.io/BERTopic/index.html)."
      ],
      "metadata": {
        "id": "VG5Z-hvZ9GIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enabling the GPU**\n",
        "\n",
        "We will use the GPU provided by COLAB to accelarate our model training. To enable GPUs for the notebook:\n",
        "1- Navigate to Edit -> Notebook Settings\n",
        "2- Select GPU from the Hardware Accelerator drop-down"
      ],
      "metadata": {
        "id": "W_0vnbvROpuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify if GPU is enable\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "gj08UQrDOqW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193abc43-e3ec-4d45-db29-febb1eedaa10"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan  5 03:46:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    33W /  70W |   3020MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**"
      ],
      "metadata": {
        "id": "eXD8VBix7DD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyspellchecker\n",
        "!pip install bertopic\n",
        "!pip install kaleido # for save BERTopic plots as image"
      ],
      "metadata": {
        "id": "csLlF-UUb2il"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "CQ0hDdDm6gtj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # for data manipulation\n",
        "import os # for interacting with the operating system\n",
        "import nltk # for natural language processing\n",
        "import string # for string manipulation\n",
        "import re # for for regular expressions\n",
        "import matplotlib.pyplot as plt # for visualization\n",
        "import spacy # for lemmatize portuguese text\n",
        "from bertopic import BERTopic # for topic modeling\n",
        "from spellchecker import SpellChecker # for spell check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spacy pt_core_news_sm for portuguese text\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "dafLrQKmvlGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae9002a-07ea-4556-cbf4-d1d7706b5d3c"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pt-core-news-sm==3.4.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
            "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from pt-core-news-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.25.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "BvRCi7Cvp0G6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0fc960-1453-4a69-ee3b-bbfa25c1e0cf"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset with stopwords\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "JAvaPg5ep3Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc006d06-b4f1-45c1-dad0-66b3ee4539d0"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload datasets for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "97RrmPshvcmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ee31a0-474e-4006-f21a-3aeffe24c861"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload dependency need to stem portuguese text\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "id": "qn9OrF3EXCNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38861ac3-b496-45a5-8661-abf4c5ce7de5"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load data from [Github](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui.git)**"
      ],
      "metadata": {
        "id": "EGjKdT4C8RVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/punkmic/Topic-Modeling-Reclame-Aqui.git"
      ],
      "metadata": {
        "id": "C_yss1JC-8Ou",
        "outputId": "20bd1659-030b-4f92-c436-35491eb47f67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Topic-Modeling-Reclame-Aqui' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory\n",
        "%cd /content/Topic-Modeling-Reclame-Aqui \n",
        "\n",
        "# Update files from remote repository\n",
        "!git pull \n",
        "\n",
        "# Return to work directory\n",
        "%cd ..\n",
        "\n",
        "# Check current directory\n",
        "!pwd"
      ],
      "metadata": {
        "id": "uv3QBnDRHFPv",
        "outputId": "e307bbd3-de56-4039-e7f9-058aa98a839f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Topic-Modeling-Reclame-Aqui\n",
            "Already up to date.\n",
            "/content\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prepare data**"
      ],
      "metadata": {
        "id": "5YitWNVVJTOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path_csv, drop_duplicates = True, lower=True):\n",
        " \n",
        "  # Use the read_csv method to read csv file\n",
        "  df = pd.read_csv(path_csv)\n",
        "  \n",
        "  if drop_duplicates:\n",
        "    # Read and return the CSV file using the read_csv method\n",
        "    print(f\"Shape before remove duplicates: {df.shape}\")\n",
        "\n",
        "    # Use the drop_duplicated method to drop duplicates rows\n",
        "    df = df.drop_duplicates(subset=\"text\")\n",
        "\n",
        "    print(f\"Shape after remove duplicates: {df.shape}\")\n",
        "\n",
        "    if lower:\n",
        "      # apply the str.lower() method to each element in the dataframe\n",
        "      df = df.applymap(str.lower)\n",
        "  return df"
      ],
      "metadata": {
        "id": "F3IoOKARBwVb"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to data\n",
        "path_csv = \"/content/Topic-Modeling-Reclame-Aqui/corpus.csv\"\n",
        "\n",
        "df = read_data(path_csv)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "2ppvYJG90iVk",
        "outputId": "06d23912-c798-412c-eaae-de2b9003b497"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before remove duplicates: (12760, 2)\n",
            "Shape after remove duplicates: (10510, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  pedido cancelado sem justificativa após uma se...   \n",
              "1                                  pedido cancelado    \n",
              "2                                  cobrança indevida   \n",
              "3                                 pedido reincidente   \n",
              "4            assinatura para vender na amazon brasil   \n",
              "\n",
              "                                                text  \n",
              "0  eu estava pesquisando bastante uma nova tv par...  \n",
              "1  eu sinceramente estou decepcionada com o amazo...  \n",
              "2  cancelei meu plano antes de terminar o período...  \n",
              "3  olha fiz compra veio errada, e veio errado nov...  \n",
              "4  eu me inscrevi na amazon para realizar vendas ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d495e767-23a8-4f8f-ae6d-d331fc05b370\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pedido cancelado sem justificativa após uma se...</td>\n",
              "      <td>eu estava pesquisando bastante uma nova tv par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pedido cancelado</td>\n",
              "      <td>eu sinceramente estou decepcionada com o amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cobrança indevida</td>\n",
              "      <td>cancelei meu plano antes de terminar o período...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pedido reincidente</td>\n",
              "      <td>olha fiz compra veio errada, e veio errado nov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>assinatura para vender na amazon brasil</td>\n",
              "      <td>eu me inscrevi na amazon para realizar vendas ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d495e767-23a8-4f8f-ae6d-d331fc05b370')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d495e767-23a8-4f8f-ae6d-d331fc05b370 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d495e767-23a8-4f8f-ae6d-d331fc05b370');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains just two columns called title and text "
      ],
      "metadata": {
        "id": "gDfB_6inJgr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "Ytly9hiGJzFA",
        "outputId": "5412b746-8790-4ddc-d1ef-a8fc96ac6f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10510, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 10510 unique rows in this dataset."
      ],
      "metadata": {
        "id": "Upvg5pKOJ0bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join columns\n",
        "df.columns = [\"title\", \"documents\"] \n",
        "\n",
        "# Use the replace() method to replace the string with an empty string\n",
        "df = df.replace(re.compile('\\[editado pelo reclame aqui\\]|editado pelo reclame aqui|Editado pelo Reclame Aqui'), '')\n",
        "\n",
        "# Drop the old index column\n",
        "df.reset_index(inplace = True, drop = True)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "N0iqgleUNVlE",
        "outputId": "c617f1e4-cff2-4ef7-9901-87ab0effb68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  pedido cancelado sem justificativa após uma se...   \n",
              "1                                  pedido cancelado    \n",
              "2                                  cobrança indevida   \n",
              "3                                 pedido reincidente   \n",
              "4            assinatura para vender na amazon brasil   \n",
              "\n",
              "                                           documents  \n",
              "0  eu estava pesquisando bastante uma nova tv par...  \n",
              "1  eu sinceramente estou decepcionada com o amazo...  \n",
              "2  cancelei meu plano antes de terminar o período...  \n",
              "3  olha fiz compra veio errada, e veio errado nov...  \n",
              "4  eu me inscrevi na amazon para realizar vendas ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c2def71-8b7d-4ca7-9913-cb0fb4e19f61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>documents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pedido cancelado sem justificativa após uma se...</td>\n",
              "      <td>eu estava pesquisando bastante uma nova tv par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pedido cancelado</td>\n",
              "      <td>eu sinceramente estou decepcionada com o amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cobrança indevida</td>\n",
              "      <td>cancelei meu plano antes de terminar o período...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pedido reincidente</td>\n",
              "      <td>olha fiz compra veio errada, e veio errado nov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>assinatura para vender na amazon brasil</td>\n",
              "      <td>eu me inscrevi na amazon para realizar vendas ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c2def71-8b7d-4ca7-9913-cb0fb4e19f61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c2def71-8b7d-4ca7-9913-cb0fb4e19f61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c2def71-8b7d-4ca7-9913-cb0fb4e19f61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save table as image**"
      ],
      "metadata": {
        "id": "J5KS9IgNbqGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = '/content/Topic-Modeling-Reclame-Aqui/results/table/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.head(10).to_csv(path + 'table.csv')"
      ],
      "metadata": {
        "id": "ziN54QGkbtk6"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing**"
      ],
      "metadata": {
        "id": "drjuwA5yk4YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tokenization**\n",
        "\n",
        "Tokenization aims to breaking text down into its component parts"
      ],
      "metadata": {
        "id": "i9ukWCy8qBOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_TOKENIZER = nltk.tokenize.word_tokenize\n",
        "def tokenize(text, lowercase=True):\n",
        "  if lowercase:\n",
        "    text = text.lower()\n",
        "  return [token.strip() for token in WORD_TOKENIZER(text, language=\"portuguese\")]"
      ],
      "metadata": {
        "id": "k_f8hsxJk7v4"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Stem** \n",
        "\n",
        "Stem the tokens. This step aims to remove morphological affixes and normalize to standardized stem forms"
      ],
      "metadata": {
        "id": "Lm7V--yMq98r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEMMER = nltk.stem.RSLPStemmer()\n",
        "def stem(tokens):\n",
        "  return [STEMMER.stem(token) for token in tokens]"
      ],
      "metadata": {
        "id": "TphgFQIxrPMM"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Lemmatize**\n",
        "\n",
        "Lemmatize the tokens. Retains more natural forms than stemming, but assumes all tokens nons unless tokens are passed as (word, pos) tuples. Note: nltk lemmatize does not suport portugues language"
      ],
      "metadata": {
        "id": "RTiUcqzrrdiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEMMATIZER = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(tokens):\n",
        "  lemmas = []\n",
        "  for token in tokens:\n",
        "      if isinstance(token, str):\n",
        "          # treats token like a noun\n",
        "          lemmas.append(LEMMATIZER.lemmatize(token)) \n",
        "      else: \n",
        "          # assume a tuple of (word, pos)\n",
        "          lemmas.append(LEMMATIZER.lemmatize(*token))\n",
        "  return lemmas"
      ],
      "metadata": {
        "id": "4O48ihphrxy3"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatize option for portuguese text**"
      ],
      "metadata": {
        "id": "eHdBPARlU24i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_pt(tokens, nlp):\n",
        "  # Create a spaCy Doc object and apply the lemmatization\n",
        "  doc = nlp(' '.join(tokens))\n",
        "\n",
        "  # Return lemmatize\n",
        "  return [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "pfgdVUHVU6Zy"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove stopwords**\n",
        "\n",
        "Stop words are things like articles and conjunctions that usually do not offer a lot of value in an analysis."
      ],
      "metadata": {
        "id": "jkLYqJgDs4Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stop_words = ['amazon', 'americanas', 'casas bahia', 'magazine luiza', 'shein', 'kabum',\n",
        "                       'samsung', 'mercado livre', 'banco brasil', 'apple', 'eletrolux']\n",
        "                       \n",
        "def remove_stopwords(tokens, stopwords=None, custom_stop_words = []):\n",
        "\n",
        "  # Use the default stop words if none is passed\n",
        "  if stopwords is None:\n",
        "    stopwords = nltk.corpus.stopwords.words('portuguese') + custom_stop_words\n",
        "  \n",
        "  # Filter the list of tokens to exclude the stop word tokens\n",
        "  return [token for token in tokens if token not in stopwords]"
      ],
      "metadata": {
        "id": "D9blZaBYtHgj"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_stopwords(['casas bahia', 'amazon', 'roupa'], custom_stop_words=custom_stop_words) == ['roupa']"
      ],
      "metadata": {
        "id": "-E4DqCin2HDc"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove hyperlinks**\n",
        "\n",
        "Removes http/s links from the tokens."
      ],
      "metadata": {
        "id": "X4qMKyfKtaAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_links(tokens):\n",
        "  # Filter tokens that starts with \"http://\" or \"https://\"\n",
        "  return [token for token in tokens \n",
        "          if not token.startswith(\"http://\")\n",
        "          and not token.startswith(\"https://\")]"
      ],
      "metadata": {
        "id": "LqlMta5qtovS"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_links(['bom', 'http://online', 'https://offline']) == ['bom']"
      ],
      "metadata": {
        "id": "vxvsG7s_6wkd"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove numbers**"
      ],
      "metadata": {
        "id": "EwKFluprz-pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(tokens):\n",
        "  # Filter out number tokens using a list comprehension and the isnumeric method\n",
        "  return [token for token in tokens if not token.isnumeric()]"
      ],
      "metadata": {
        "id": "ALKnXv8C0EWu"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_numbers(['ola', 'bicicleta', '1', '2002']) == ['ola', 'bicicleta']"
      ],
      "metadata": {
        "id": "32uzYfBG3cv2"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove date**"
      ],
      "metadata": {
        "id": "a0pZKAQ7ZJHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_date(tokens):\n",
        "  # Compile a regular expression to match dates in the format dd/mm or dd/mm/yyyy\n",
        "  date_regex = re.compile(r'\\d{2}/\\d{2}(/\\d{4})?')\n",
        "\n",
        "  # Use the regex to find all the tokens that match the date pattern\n",
        "  dates = [token for token in tokens if date_regex.fullmatch(token)]\n",
        "\n",
        "  # Filter the list of tokens to exclude the date tokens\n",
        "  filtered_tokens = [token for token in tokens if token not in dates]\n",
        "\n",
        "  # Return the filtered tokens\n",
        "  return filtered_tokens"
      ],
      "metadata": {
        "id": "ywykIjKZctc-"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_date(['texto', 'data', '20/10', 'seguro', '02/09/2014']) == ['texto', 'data', 'seguro']"
      ],
      "metadata": {
        "id": "k6r3cMbB7F3f"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove punctuation**"
      ],
      "metadata": {
        "id": "7y1FqCD1t75_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(tokens,\n",
        "                       strip_mentions=True,\n",
        "                       strip_hashtags=True):\n",
        "\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "\n",
        "    # Filter punctuation tokens\n",
        "    tokens = [token.strip() for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Remove @ symbol from left side of tokens\n",
        "    if strip_mentions:\n",
        "        tokens = [t.lstrip(r\"([!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~])\\1+\") for t in tokens]\n",
        "\n",
        "    # Remove # symbol from left side of tokens\n",
        "    if strip_hashtags:\n",
        "        tokens = [t.lstrip(r\"([!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~])\\1+\") for t in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "ba7paG7_uBtS"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_punctuation(['limpo', 'acento/  ///', 'simples???', 'onde', ',']) == ['limpo', 'acento', 'simples', 'onde']"
      ],
      "metadata": {
        "id": "7E8n6Z0l7Xe_"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove short tokens**"
      ],
      "metadata": {
        "id": "SiUx2T8C5YBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_tokens(tokens):\n",
        "  # Filter the list of tokens to exclude tokens that are shorter than four letters\n",
        "  filtered_tokens = [token for token in tokens if len(token) >= 4]\n",
        "\n",
        "  # Return the filtered tokens\n",
        "  return filtered_tokens"
      ],
      "metadata": {
        "id": "t8SOwNYW6QI1"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_short_tokens(['sair', 'um', 'correto', 'igual', 'oi']) == ['sair', 'correto', 'igual']"
      ],
      "metadata": {
        "id": "GTF9de9N7uFK"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Correction of spelling errors**"
      ],
      "metadata": {
        "id": "WqtOisN38pZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_spell_errors(text, spell):\n",
        "\n",
        "  # Correct the spelling errors in the text\n",
        "  corrected_text = spell.correction(text)\n",
        "\n",
        "  # If no correction is present user the original text\n",
        "  if corrected_text == None:\n",
        "     corrected_text =  text\n",
        "  \n",
        "  # Return the corrected text\n",
        "  return corrected_text"
      ],
      "metadata": {
        "id": "fUTiJYc_8hba"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(documents, nlp = None, spell = None):\n",
        "  corpus = []\n",
        "\n",
        "  # process each document and append to corpus list\n",
        "  for i, text in enumerate(documents):\n",
        "    if i % 1000 == 0:\n",
        "      print(f\"{i} documents of {len(documents)}\\n\")\n",
        "    if spell is not None:\n",
        "      text = check_spell_errors(text, spell)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = remove_links(tokens)\n",
        "    tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n",
        "    tokens = remove_numbers(tokens)\n",
        "    tokens = remove_date(tokens)\n",
        "    if nlp is not None: \n",
        "      tokens = lemmatize_pt(tokens, nlp)\n",
        "    #tokens = stem(tokens) \n",
        "    tokens = remove_short_tokens(tokens)\n",
        "    tokens = remove_stopwords(tokens,custom_stop_words=custom_stop_words)\n",
        "    corpus.append(' '.join(tokens))\n",
        "  return corpus"
      ],
      "metadata": {
        "id": "5aiFtbo3uuh_"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SpellChecker object\n",
        "spell = SpellChecker(language='pt')\n",
        "\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "corpus = preprocessing(df.documents, nlp, spell)"
      ],
      "metadata": {
        "id": "Tu75mPgywHwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a263be-10d0-4626-c59b-d6d40faf465d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 documents of 10510\n",
            "\n",
            "1000 documents of 10510\n",
            "\n",
            "2000 documents of 10510\n",
            "\n",
            "3000 documents of 10510\n",
            "\n",
            "4000 documents of 10510\n",
            "\n",
            "5000 documents of 10510\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first document before and after pre-processing it\n",
        "print('Before preprocessing')\n",
        "print(df.documents[0])\n",
        "print('\\nAfter preprocessig')\n",
        "print(corpus[0])"
      ],
      "metadata": {
        "id": "3jmOFKM1M-Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training a BERTopic Model**\n",
        "\n",
        "The BERTopic algorithm has several advantages over other topic modeling algorithms. It is able to handle sparse data, it is scalable to large datasets, and it is able to learn topics that are not well-defined or are overlapping.\n",
        "\n",
        "As our data language is portuguese we will going to set language to multilingual."
      ],
      "metadata": {
        "id": "AWmOFQ3VOD05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new BERTopic model and train it. By default BERTopic use the paraphrase-multilingual-MiniLM-L12-v2 model for multi language documents. For others model check here [BERTopic sentence transformers](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html#sentence-transformers)"
      ],
      "metadata": {
        "id": "_mBsnRnhgbm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new BERTopic model using multilingual option\n",
        "topic_model = BERTopic(language=\"multilingual\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Train model \n",
        "topics, probs = topic_model.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "B0EFqnutN2Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTopic works in three main steps: \n",
        "\n",
        "\n",
        "1.   Documents are first converted to numeric data. It extracts different embeddings based on the context of the word. For this, a sentence transformation model is used.\n",
        "2.  Documents with similar topics are then grouped together forming clusters with similar topics. For this purpose, BERTopic uses the clustering algorithm UMAP to lower the dimensionality of the embeddings. Then the documents are clustered with the density-based algorithm HDBSCAN.\n",
        "3. BERTopic extracts topics from clusters using a class-based TF-IDF score. This score gives the importance of each word in a cluster. Topics are then created based on the most important words measured by their C-TF-IDF score.\n",
        "\n",
        "For more information check this link [BERTopic](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)\n",
        "\n"
      ],
      "metadata": {
        "id": "vnG5aC6ekXeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting Topics**"
      ],
      "metadata": {
        "id": "GimKIMmaRZSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the most frequent topics\n",
        "freq = topic_model.get_topic_info()\n",
        "\n",
        "# Show the top 5 most frequent topics\n",
        "freq.head(5)"
      ],
      "metadata": {
        "id": "JIrBNmF-Rcf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table above shows the five most freqeuente topics and the words present on it extract by BERTopic. -1 refers to all outliers and should be ignored."
      ],
      "metadata": {
        "id": "TVWcfF5mRrMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show the most frequent topic\n",
        "topic_model.get_topic(1)"
      ],
      "metadata": {
        "id": "Br_UMf1kRxU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** BERTopic is stocastich which means that the topics might differ across runs this is mostly due to the stocastisch nature of UMAP"
      ],
      "metadata": {
        "id": "YpP4HgF2R6Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save topic info table as CSV**"
      ],
      "metadata": {
        "id": "CxWLChzJlPmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = '/content/Topic-Modeling-Reclame-Aqui/results/topic_info_tables/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "# Save table as csv\n",
        "freq.head(10).to_csv(path + 'topic_info_preprocessed_lemma.csv')"
      ],
      "metadata": {
        "id": "9wM4Ymh_lOdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualization**"
      ],
      "metadata": {
        "id": "wBsA8hiVSJDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Intertopic Distance Map**\n",
        "\n",
        "This graph shows the distance intertopic and help us understand the promixity of topics"
      ],
      "metadata": {
        "id": "2twooZDXUc2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_topics()\n",
        "fig"
      ],
      "metadata": {
        "id": "Ae2JSAENSIOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save intertopic distance map**"
      ],
      "metadata": {
        "id": "VEy11A_SlliP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = '/content/Topic-Modeling-Reclame-Aqui/results/intertopic_distance_map/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"idm_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "hLy_Eeu4lqCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/intertopic_distance_map/idm_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "Jf-iwAsQoOCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Topic Hierarchy**\n",
        "\n",
        "The topics that were created can be hierarchically reduced. This visualization shows how the topics relate to one another."
      ],
      "metadata": {
        "id": "r-5bT7AgSgsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_hierarchy(top_n_topics=30)\n",
        "fig"
      ],
      "metadata": {
        "id": "TEoOpDD6S4D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save Hierarchical Clustering**"
      ],
      "metadata": {
        "id": "CyM_L5c6pBXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = '/content/Topic-Modeling-Reclame-Aqui/results/hierarchical_clustering/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"hc_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "CHYmk_TZpHPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/hierarchical_clustering/hc_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "VrqyRupApBx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Terms**\n",
        "\n",
        "We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation."
      ],
      "metadata": {
        "id": "uC26Cc-6UvYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_barchart(top_n_topics=12)\n",
        "fig"
      ],
      "metadata": {
        "id": "-NoMSyeWVF2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save Top Word Scores Bar Chart**"
      ],
      "metadata": {
        "id": "MZWFicVqpWUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = '/content/Topic-Modeling-Reclame-Aqui/results/top_words_scores/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"tws_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "Qrz2ywMapdO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/top_words_scores/tws_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "HtFLmwl4pTAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Topic Similarity**\n",
        "\n",
        "This plot shows a similarity matrix by simply applying cosine similarities through those topic embeddings generate by BERTopic through both c-TF-IDF and embeddings. This matrix indicate how similar certain topics are to each other."
      ],
      "metadata": {
        "id": "qMjOvmYVVUDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_heatmap(n_clusters=20, width=1000, height=1000)\n",
        "fig"
      ],
      "metadata": {
        "id": "_GL5qZ0nV3M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save Similarity Matrix**"
      ],
      "metadata": {
        "id": "CF3QJy6epmQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = '/content/Topic-Modeling-Reclame-Aqui/results/similarity_matrix/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"sm_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "VQOAZurPprA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/similarity_matrix/sm_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "y1pa1kWApYgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Term Score Decline**\n",
        "\n",
        "Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added."
      ],
      "metadata": {
        "id": "vd41xsx7Xhgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = topic_model.visualize_term_rank()\n",
        "fig"
      ],
      "metadata": {
        "id": "Rw1wXTEtX967"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save Term score decline per Topic**"
      ],
      "metadata": {
        "id": "6UXg4gpFpwZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to save \n",
        "path = '/content/Topic-Modeling-Reclame-Aqui/results/term_score_decline_topic/'\n",
        "\n",
        "# Use makedirs() to create a new directory if it does not exists\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "\n",
        "fig.write_image(path + \"tsdp_preprocessed_lemma.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "F5Gppkjpp0hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Github link](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/results/term_socore_decline_topic/tsdp_preprocessed_lemma.png?raw=true)"
      ],
      "metadata": {
        "id": "QERxP9EspLjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Term search**"
      ],
      "metadata": {
        "id": "AZIV-p12ZMXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find topics that contains blackfriday term\n",
        "similar_topics, similarity = topic_model.find_topics(\"blackfriday\", top_n=5)\n",
        "\n",
        "# Show similar topics\n",
        "similar_topics"
      ],
      "metadata": {
        "id": "pHHayMGRZOob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a specific topic\n",
        "topic_model.get_topic(34)"
      ],
      "metadata": {
        "id": "2BnwvRotZUya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hiperparameter optimization**"
      ],
      "metadata": {
        "id": "f3Q_mQRNPRhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install optuna\n",
        "!pip install hdbscan\n",
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "5vxgob5fPpc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import optuna\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "import ast"
      ],
      "metadata": {
        "id": "qQOTWqh-PTwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_documents(model, corpus, trial, reduced):\n",
        "  fig = topic_model.visualize_documents(corpus, hide_document_hover=True, hide_annotations=True)\n",
        "\n",
        "  # Set the path to save \n",
        "  path = '/content/Topic-Modeling-Reclame-Aqui/results/documents_n_topics/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"document_n_topics_trial_{trial}.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "O-l1u6wucN1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_topics(model, trial, reduced=False):\n",
        "  fig = model.visualize_topics()\n",
        "\n",
        "  # Set the path to save \n",
        "  path = '/content/Topic-Modeling-Reclame-Aqui/results/intertopic_distance_map/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"intertopic_distance_map_trial_{trial}.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "zEr6GcxHZptw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_hierarchy(model, trial, reduced=False):\n",
        "  fig = model.visualize_hierarchy()\n",
        "\n",
        "  # Set the path to save \n",
        "  path = '/content/Topic-Modeling-Reclame-Aqui/results/hierarchical_clustering/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"hierarchical_clustering_trial_{trial}.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "9XqbXn0KZ3r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_top_words_scores(model, trial, reduced=False):\n",
        "  fig = model.visualize_barchart()\n",
        "\n",
        "  # Set the path to save \n",
        "  path = '/content/Topic-Modeling-Reclame-Aqui/results/top_words_scores/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"top_words_scores_trial_{trial}.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "A_21t9rmZ4Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_similarity_matrix(model, trial, reduced=False):\n",
        "  fig = model.visualize_heatmap()\n",
        "\n",
        "  # Set the path to save \n",
        "  path = '/content/Topic-Modeling-Reclame-Aqui/results/similarity_matrix/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"similarity_matrix_trial_{trial}.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "lTV5l7q7Z45m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_term_rank(model, trial, reduced=False):\n",
        "  fig = model.visualize_term_rank()\n",
        "\n",
        "  # Set the path to save \n",
        "  path = '/content/Topic-Modeling-Reclame-Aqui/results/term_score_decline_topic/'\n",
        "\n",
        "  if reduced:\n",
        "    path = path + 'reduced/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  fig.write_image(path + f\"term_score_trial_{trial}.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "Ag4Yv6r2bfeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_hyperparameters(trial_params, label):\n",
        "  # Set the path to save \n",
        "  path = '/content/Topic-Modeling-Reclame-Aqui/results/hyperparameters/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  with open(path + f\"hyperparameters_trial_{label}.json\", \"w\") as f:\n",
        "    f.write(json.dumps(trial_params))\n"
      ],
      "metadata": {
        "id": "3wuSDGh4dxlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizer(trial):\n",
        "\n",
        "  # BERTopic hyperparameters\n",
        "  top_n_words = trial.suggest_int('bertopic__top_n_words', 5, 10)\n",
        "  n_gram_range = ast.literal_eval(trial.suggest_categorical('bertopic__n_gram_range', ['(1,1)', '(1,2)', '(1,3)']))\n",
        "  min_topic_size = trial.suggest_int('bertopic__min_topic_size', 15, 20)\n",
        "  diversity = trial.suggest_int('bertopic__diversity', 0.1, 0.9)\n",
        "  outlier_threshold = trial.suggest_float('bertopic__outliers_threshold', 0.01, 0.09)\n",
        "  nr_topics = trial.suggest_int('bertopic__nr_topics', 12, 20)\n",
        "\n",
        "  # UMAP hyperparameters\n",
        "  n_neighbors = trial.suggest_int('umap__n_neighbors', 10, 20)\n",
        "  #n_components = trial.suggest_int('umap__n_components', 2, 6)\n",
        "  metric = trial.suggest_categorical('umap__metric', ['cosine', 'euclidean'])\n",
        "  min_dist = trial.suggest_float('umap__min_dist', 0.1, 1.0)\n",
        "  spread = trial.suggest_float('umap__spread', 0.1, 1.0)\n",
        "\n",
        "  # HDBSCAN hyperprarameters\n",
        "  min_cluster_size = trial.suggest_int('hdbscan__min_cluster_size', 10, 20)\n",
        "  min_samples = trial.suggest_int('hdbscan__min_samples', 10, 20)\n",
        "  cluster_selection_epsilon = trial.suggest_float('hdbscan__cluster_selection_epsilon', 0.1, 1.0)\n",
        "\n",
        "  # CountVectorizer hyperparameters \n",
        "  max_features = trial.suggest_int('vectorizer__max_features', 3000, 8000)\n",
        "\n",
        "  # reduce the impact of frequent words.\n",
        "  ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "\n",
        "  stop_words = nltk.corpus.stopwords.words('portuguese') + custom_stop_words\n",
        "  vectorizer_model = CountVectorizer(stop_words=stop_words, ngram_range=n_gram_range, max_features=max_features)\n",
        "\n",
        "  umap_model = UMAP(n_neighbors=n_neighbors, metric=metric, random_state=42)\n",
        "\n",
        "  hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
        "\n",
        "  # create a new BERTopic model using multilingual option\n",
        "  model = BERTopic(language=\"multilingual\", \n",
        "                         calculate_probabilities=True, \n",
        "                         verbose=False,\n",
        "                         nr_topics='auto',\n",
        "                         top_n_words=top_n_words,\n",
        "                         n_gram_range=n_gram_range,\n",
        "                         min_topic_size=min_topic_size,\n",
        "                         diversity=diversity,\n",
        "                         umap_model=umap_model,\n",
        "                         hdbscan_model=hdbscan_model,\n",
        "                         vectorizer_model=vectorizer_model,\n",
        "                         ctfidf_model=ctfidf_model)\n",
        "  \n",
        "  label = trial.number\n",
        "  params = trial.params\n",
        "\n",
        "  # train model \n",
        "  topics, probs = model.fit_transform(corpus)\n",
        "\n",
        "  # save plots\n",
        "  save_topics(model, label, False)\n",
        "  save_documents(model, corpus, label, False)\n",
        "  save_hierarchy(model, label, False)\n",
        "  save_similarity_matrix(model, label, False)\n",
        "  save_term_rank(model, label, False)\n",
        "  save_top_words_scores(model, label, False)\n",
        "\n",
        "  # reduce outliers\n",
        "  new_topics = model.reduce_outliers(corpus, topics, probabilities=probs, strategy=\"probabilities\", threshold=outlier_threshold)\n",
        "  model.update_topics(corpus, topics=new_topics)\n",
        "\n",
        "  # save plots after reducion\n",
        "  save_topics(model, label, True)\n",
        "  save_documents(model, corpus, label, True)\n",
        "  save_hierarchy(model, label, True)\n",
        "  save_similarity_matrix(model, label, True)\n",
        "  save_term_rank(model, label, True)\n",
        "  save_top_words_scores(model, label, True)\n",
        "\n",
        "  # save hyperparameters\n",
        "  save_hyperparameters(params, label)\n",
        "\n",
        "  # save model\n",
        "  path = '/content/Topic-Modeling-Reclame-Aqui/results/models/'\n",
        "\n",
        "  # Use makedirs() to create a new directory if it does not exists\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  model.save(path + f\"model_trial_{label}\")\n",
        "\n",
        "  return 0"
      ],
      "metadata": {
        "id": "O87BZ5kyQEvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "study = optuna.create_study(study_name='Bertopic')\n",
        "study.optimize(optimizer, n_trials=50)"
      ],
      "metadata": {
        "id": "57mnK6MfYAp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Merge of similar topics**"
      ],
      "metadata": {
        "id": "kmSbzWH0gJHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: select the best model and merge similar topic and save final model\n",
        "\n",
        "#topic_model = BERTopic.load(\"best_model\")"
      ],
      "metadata": {
        "id": "OWI9jDFYgTLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########33!rm -rf /content/Topic-Modeling-Reclame-Aqui/results"
      ],
      "metadata": {
        "id": "6f8Adw95AVkP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}