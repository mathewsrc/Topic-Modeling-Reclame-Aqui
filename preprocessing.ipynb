{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/Topic-Modeling-Reclame-Aqui/blob/master/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-Processing** "
      ],
      "metadata": {
        "id": "y2QxkuH89sQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**"
      ],
      "metadata": {
        "id": "eXD8VBix7DD1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "CQ0hDdDm6gtj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # for data manipulation\n",
        "import os # for interacting with the operating system\n",
        "import nltk # for natural language processing\n",
        "import string # for string manipulation \n",
        "import re # for for regular expressions\n",
        "import matplotlib.pyplot as plt # for visualization\n",
        "import spacy # for lemmatize portuguese text\n",
        "import pickle\n",
        "import seaborn as sns # for visualizations\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "try:\n",
        "  from spellchecker import SpellChecker # for spell check\n",
        "  from wordcloud import WordCloud\n",
        "except:\n",
        "  !pip install pyspellchecker\n",
        "  !pip install wordcloud\n",
        "  from spellchecker import SpellChecker # for spell check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install spacy pt_core_news_sm for portuguese text\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "dafLrQKmvlGD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# provides a set of unsupervised algorithms that can be used for tokenization\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "BvRCi7Cvp0G6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b87b2b0-fd82-41cf-c503-6607bbf74885"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset with stopwords\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "JAvaPg5ep3Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a945b90-67b8-41a3-b181-c0fa50dd69cd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload datasets for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "97RrmPshvcmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7e5f3b-fdd6-4b11-f435-c05e259557ee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload dependency need to stem portuguese text\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "id": "qn9OrF3EXCNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f42815a-5e34-4896-e727-5b3cd87ad27c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load data from [Github](https://github.com/punkmic/Topic-Modeling-Reclame-Aqui.git)**"
      ],
      "metadata": {
        "id": "EGjKdT4C8RVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/punkmic/Topic-Modeling-Reclame-Aqui.git"
      ],
      "metadata": {
        "id": "C_yss1JC-8Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory\n",
        "%cd /content/Topic-Modeling-Reclame-Aqui \n",
        "\n",
        "# Update files from remote repository\n",
        "!git pull \n",
        "\n",
        "# Check current directory\n",
        "!pwd"
      ],
      "metadata": {
        "id": "uv3QBnDRHFPv",
        "outputId": "55abd41b-4f98-44dc-81af-2827fd799070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Topic-Modeling-Reclame-Aqui\n",
            "Warning: Permanently added the RSA host key for IP address '140.82.113.3' to the list of known hosts.\n",
            "Already up to date.\n",
            "/content/Topic-Modeling-Reclame-Aqui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORK_DIR = '/content/Topic-Modeling-Reclame-Aqui/datasets'"
      ],
      "metadata": {
        "id": "AXc0CHKwGrm7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path_csv, drop_duplicates = True, lower=True):\n",
        " \n",
        "  # use the read_csv method to read csv file\n",
        "  df = pd.read_csv(path_csv)\n",
        "  \n",
        "  if drop_duplicates:\n",
        "    # read and return the CSV file using the read_csv method\n",
        "    print(f\"Shape before remove duplicates: {df.shape}\")\n",
        "\n",
        "    # use the drop_duplicated method to drop duplicates rows\n",
        "    df = df.drop_duplicates(subset=\"text\")\n",
        "\n",
        "    print(f\"Shape after remove duplicates: {df.shape}\")\n",
        "\n",
        "    if lower:\n",
        "      # apply the str.lower() method to each element in the dataframe\n",
        "      df = df.applymap(str.lower)\n",
        "    \n",
        "     # rename columns\n",
        "    df.columns = [\"title\", \"documents\"] \n",
        "\n",
        "    # use the replace() method to replace the string with an empty string\n",
        "    df = df.replace(re.compile('\\[editado pelo reclame aqui\\]|editado pelo reclame aqui|Editado pelo Reclame Aqui'), '')\n",
        "    df = df.replace(re.compile('\\[casas bahia\\]|Casa Bahia|Casas Bahia|casa bahia'), '')\n",
        "    df = df.replace(re.compile('\\[magazine luiza\\]|Magazine luiza|Magazine Luiza| Magazine luizar|Magazine Luizar'), '')\n",
        "    df = df.replace(re.compile('\\[mercado livre\\]|Mercado Livre|Mercado livre'), '')\n",
        "    df = df.replace(re.compile('\\[americana\\]|Ameriacanas|ameriacanas'), '')\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "F3IoOKARBwVb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing**"
      ],
      "metadata": {
        "id": "drjuwA5yk4YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tokenization**\n",
        "\n",
        "Tokenization aims to breaking text down into its component parts"
      ],
      "metadata": {
        "id": "i9ukWCy8qBOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_TOKENIZER = nltk.tokenize.word_tokenize\n",
        "def tokenize(text):\n",
        "  tokens = [token.strip().lower() for token in WORD_TOKENIZER(text, language=\"portuguese\")]\n",
        " \n",
        "  # set a pattern to detect patterns such as x x, xxx x, xxx xxx\n",
        "  pattern = r\"\\b\\w+\\s+\\w+\\b\"\n",
        " \n",
        "  # filter tokens by pattern\n",
        "  filtered_words = [word for word in tokens if re.search(pattern, word)]\n",
        "\n",
        "  # return token if not in filter list\n",
        "  return [token for token in tokens if token not in filtered_words]"
      ],
      "metadata": {
        "id": "k_f8hsxJk7v4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Stem** \n",
        "\n",
        "Stem the tokens. This step aims to remove morphological affixes and normalize to standardized stem forms"
      ],
      "metadata": {
        "id": "Lm7V--yMq98r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEMMER = nltk.stem.RSLPStemmer()\n",
        "def stem(text):\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "  return \" \".join([STEMMER.stem(token) for token in tokens])"
      ],
      "metadata": {
        "id": "TphgFQIxrPMM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Lemmatize**\n",
        "\n",
        "Lemmatize the tokens. Retains more natural forms than stemming, but assumes all tokens nons unless tokens are passed as (word, pos) tuples. Note: nltk lemmatize does not suport portugues language"
      ],
      "metadata": {
        "id": "RTiUcqzrrdiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEMMATIZER = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(text):\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "  lemmas = []\n",
        "  for token in tokens:\n",
        "      if isinstance(token, str):\n",
        "          # treats token like a noun\n",
        "          lemmas.append(LEMMATIZER.lemmatize(token)) \n",
        "      else: \n",
        "          # assume a tuple of (word, pos)\n",
        "          lemmas.append(LEMMATIZER.lemmatize(*token))\n",
        "  return \" \".join(lemmas)"
      ],
      "metadata": {
        "id": "4O48ihphrxy3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatize option for portuguese text**"
      ],
      "metadata": {
        "id": "eHdBPARlU24i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load portuguese model\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "def lemmatize_pt(text):\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "  # Create a spaCy Doc object and apply the lemmatization\n",
        "  doc = nlp(' '.join(tokens))\n",
        "\n",
        "  # Return lemmatize\n",
        "  return \" \".join([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "id": "pfgdVUHVU6Zy"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove stopwords**\n",
        "\n",
        "Stop words are things like articles and conjunctions that usually do not offer a lot of value in an analysis."
      ],
      "metadata": {
        "id": "jkLYqJgDs4Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stop_words = ['amazon', 'americanas', 'casas bahia', 'magazine luiza', 'shein', 'kabum',\n",
        "                       'samsung', 'mercado livre', 'banco brasil', 'apple', 'magazine', 'luiza', 'luizar',\n",
        "                      'casas', 'bahia', 'casa', 'mercado', 'livre','magalu']"
      ],
      "metadata": {
        "id": "TW2aSpqiodwr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, stopwords=None, custom_stop_words = None):\n",
        "\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "  \n",
        "  if custom_stop_words is None:\n",
        "    custom_stop_words = ['amazon', 'americanas', 'casas bahia', 'magazine luiza', 'shein', 'kabum',\n",
        "                       'samsung', 'mercado livre', 'banco brasil', 'apple', 'magazine', 'luiza', 'luizar',\n",
        "                      'casas', 'bahia', 'casa', 'mercado', 'livre']\n",
        "\n",
        "  # Use the default stop words if none is passed\n",
        "  if stopwords is None:\n",
        "    stopwords = nltk.corpus.stopwords.words('portuguese') + custom_stop_words    \n",
        "  \n",
        "  # Filter the list of tokens to exclude the stop word tokens\n",
        "  return \" \".join([token for token in tokens if token not in stopwords])"
      ],
      "metadata": {
        "id": "D9blZaBYtHgj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_stopwords(['compra', 'echar', 'em esse', 'amazon', 'pude'], custom_stop_words=custom_stop_words) == 'compra echar em esse pude'"
      ],
      "metadata": {
        "id": "-E4DqCin2HDc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove frequent words**"
      ],
      "metadata": {
        "id": "-QWYIcK4jIsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_frequent_words(text, **kwargs):\n",
        "  \"\"\"\n",
        "  df: DataFrame\n",
        "  colname: DataFrame column name\n",
        "  top_words: top most frequent words to remove\n",
        "  \"\"\"\n",
        "\n",
        "  df = kwargs['df']\n",
        "  colname = kwargs['colname']\n",
        "  top_words = kwargs['top_words']\n",
        "  \n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "\n",
        "  # Tokenize the text and store them in a new column\n",
        "  df['words'] = df[colname].apply(WORD_TOKENIZER)\n",
        "\n",
        "  # Flatten the list of words\n",
        "  all_words = [word for sublist in df['words'] for word in sublist]\n",
        "  \n",
        "  # filter tokens by its frequency\n",
        "  freq = nltk.FreqDist(all_words).most_common(top_words)\n",
        "  words = [item[0] for item in freq]\n",
        "\n",
        "  pd.DataFrame(freq, \n",
        "               columns=[\"words\", \"frequency\"]).to_csv(\n",
        "                   os.path.join(WORK_DIR,'frequent_words_removed.csv'), index=False)\n",
        "  return \" \".join([token for token in tokens if token not in words])"
      ],
      "metadata": {
        "id": "enKPv_nbjLIO"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame({'documents':['we remove the punctuation, numbers, and stop words from each commit message.',\n",
        "                                     'Random Forrest does a better job when you have a multi-class problem',\n",
        "                                     'BentoML is an all-in-one framework to maintain, package and deploy models of any framework'] })\n",
        "\n",
        "assert remove_frequent_words(['we', 'is', 'numbers', 'job', 'maintain'], df=df_test, colname='documents', top_words=5) == 'is numbers job maintain'"
      ],
      "metadata": {
        "id": "lnnemdvDjc1P"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove hyperlinks**\n",
        "\n",
        "Removes http/s links from the tokens."
      ],
      "metadata": {
        "id": "X4qMKyfKtaAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_links(text):\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = text.split()\n",
        "  # Filter tokens that starts with \"http://\" or \"https://\"\n",
        "  return \" \".join([token for token in tokens \n",
        "          if not token.startswith(\"http://\")\n",
        "          and not token.startswith(\"https://\")])"
      ],
      "metadata": {
        "id": "LqlMta5qtovS"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_links(['bom', 'http://online', 'https://offline']) == 'bom'"
      ],
      "metadata": {
        "id": "vxvsG7s_6wkd"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove numbers**"
      ],
      "metadata": {
        "id": "EwKFluprz-pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "  # Use a regular expression to match words that contain numbers\n",
        "  pattern = r\"\\b\\w*\\d\\w*\\b\"\n",
        "  tokens = [token for token in tokens if not re.sub(pattern, \"\", token) == \"\"]\n",
        "  \n",
        "  # Filter out number tokens using a list comprehension and the isnumeric method\n",
        "  return \" \".join([token for token in tokens if not token.isnumeric()])"
      ],
      "metadata": {
        "id": "ALKnXv8C0EWu"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_numbers(['ola', 'bicicleta', '1', '2002']) == 'ola bicicleta'"
      ],
      "metadata": {
        "id": "32uzYfBG3cv2"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove date**"
      ],
      "metadata": {
        "id": "a0pZKAQ7ZJHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_date(text):\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "  # Compile a regular expression to match dates in the format dd/mm or dd/mm/yyyy\n",
        "  date_regex = re.compile(r'\\d{2}/\\d{2}(/\\d{4})?')\n",
        "\n",
        "  # Use the regex to find all the tokens that match the date pattern\n",
        "  dates = [token for token in tokens if date_regex.fullmatch(token)]\n",
        "\n",
        "  # Filter the list of tokens to exclude the date tokens\n",
        "  filtered_tokens = [token for token in tokens if token not in dates]\n",
        "\n",
        "  # Return the filtered tokens\n",
        "  return \" \".join(filtered_tokens)"
      ],
      "metadata": {
        "id": "ywykIjKZctc-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_date(['texto', 'data', '20/10', 'seguro', '02/09/2014']) == 'texto data seguro'"
      ],
      "metadata": {
        "id": "k6r3cMbB7F3f"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove punctuation**"
      ],
      "metadata": {
        "id": "7y1FqCD1t75_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text,\n",
        "                       strip_mentions=True,\n",
        "                       strip_hashtags=True):\n",
        "\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "  tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "\n",
        "  # Filter punctuation tokens\n",
        "  tokens = [token.strip() for token in tokens if token not in string.punctuation]\n",
        "\n",
        "  # Remove @ symbol from left side of tokens\n",
        "  if strip_mentions:\n",
        "      tokens = [t.lstrip(r\"([!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~])\\1+\") for t in tokens]\n",
        "\n",
        "  # Remove # symbol from left side of tokens\n",
        "  if strip_hashtags:\n",
        "      tokens = [t.lstrip(r\"([!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~])\\1+\") for t in tokens]\n",
        "\n",
        "  return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "ba7paG7_uBtS"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_punctuation(['limpo', 'acento/  ///', 'simples???', 'onde', ',']) == 'limpo acento simples onde'"
      ],
      "metadata": {
        "id": "7E8n6Z0l7Xe_"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove short tokens**"
      ],
      "metadata": {
        "id": "SiUx2T8C5YBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_tokens(text):\n",
        "  tokens = text\n",
        "  if not isinstance(tokens, list):\n",
        "    tokens = tokenize(text)\n",
        "  # Filter the list of tokens to exclude tokens that are shorter than four letters\n",
        "  filtered_tokens = [token for token in tokens if len(token) >= 4]\n",
        "\n",
        "  # Return the filtered tokens\n",
        "  return \" \".join(filtered_tokens)"
      ],
      "metadata": {
        "id": "t8SOwNYW6QI1"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert remove_short_tokens(['sair', 'um', 'correto', 'igual', 'oi', 'de', 'em']) == 'sair correto igual'"
      ],
      "metadata": {
        "id": "GTF9de9N7uFK"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Correction of spelling errors**"
      ],
      "metadata": {
        "id": "WqtOisN38pZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SpellChecker object\n",
        "spell = SpellChecker(language='pt')\n",
        "\n",
        "def check_spell_errors(text):\n",
        "  result = []\n",
        "  for token in text:\n",
        "    # Correct the spelling errors in the text\n",
        "    corrected_text = spell.correction(token)\n",
        "\n",
        "    # If no correction is present user the original text\n",
        "    if corrected_text == None:\n",
        "      corrected_text =  token\n",
        "  \n",
        "    result.append(corrected_text)\n",
        "  # Return the corrected text\n",
        "  return result"
      ],
      "metadata": {
        "id": "fUTiJYc_8hba"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remove extra white spaces**"
      ],
      "metadata": {
        "id": "ybMjuBaSw67b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_whitespace(document):\n",
        "    return  \" \".join(document.split())"
      ],
      "metadata": {
        "id": "3ig6LBnhw_6L"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df, filename, check_spell = False, rm_frequent_words=False, top_words=20):\n",
        "  for colname in df.columns:\n",
        "    df[colname]= df[colname].str.lower()\n",
        "    df[colname]= df[colname].apply(remove_whitespace)\n",
        "\n",
        "    if check_spell:\n",
        "      df[colname] = df[colname].apply(check_spell_errors)\n",
        "    df[colname] = df[colname].apply(remove_links)\n",
        "    df[colname] = df[colname].apply(remove_punctuation)\n",
        "    df[colname] = df[colname].apply(remove_numbers)\n",
        "    df[colname] = df[colname].apply(remove_date)\n",
        "    df[colname] = df[colname].apply(remove_short_tokens)\n",
        "    df[colname] = df[colname].apply(remove_stopwords)\n",
        "    df[colname] = df[colname].apply(lemmatize_pt) \n",
        "    if remove_frequent_words:\n",
        "      df[colname] = df[colname].apply(remove_frequent_words,\n",
        "                                      df=df, \n",
        "                                      colname=colname,\n",
        "                                      top_words=top_words)\n",
        "    df[colname] = df[colname].apply(tokenize)\n",
        "    df[colname] = df[colname].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(os.path.join(WORK_DIR, '{}.csv'), index=False)\n",
        "  return df"
      ],
      "metadata": {
        "id": "73uvqVnafwvQ"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_csv = os.path.join(WORK_DIR, \"docs.csv\")"
      ],
      "metadata": {
        "id": "gythnkSTj_u3"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess_data(df=read_data(path_csv), filename=\"preprocessed_v1\", rm_frequent_words=False)"
      ],
      "metadata": {
        "id": "gZv0TcEMwSYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6ca426-75ae-4044-f62b-76b3582e9e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before remove duplicates: (12760, 2)\n",
            "Shape after remove duplicates: (10510, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = preprocess_data(df=read_csv(path_csv), filename=\"preprocessed_v2\", rm_frequent_words=True)"
      ],
      "metadata": {
        "id": "uylbg-eN1Eva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Most Frequent words**"
      ],
      "metadata": {
        "id": "cN-vmB2tTp2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_frequency_barchart(df, colname, filename):\n",
        "  # split the text column into words\n",
        "  df['words'] = df[colname].str.split()\n",
        "\n",
        "  # get the frequency of each word\n",
        "  word_freq = df['words'].explode().value_counts()\n",
        "\n",
        "  # get the 20 most frequent words\n",
        "  most_common_words = word_freq.head(20)\n",
        "\n",
        "  # figsize\n",
        "  plt.figure(figsize=[10,8])\n",
        "\n",
        "  # create a bar plot\n",
        "  sns.barplot(x = most_common_words.index, y = most_common_words.values)\n",
        "\n",
        "  # add labels and title\n",
        "  plt.xlabel(\"Palavras\")\n",
        "  plt.ylabel(\"Frequência\")\n",
        "  plt.title(\"20 palavras mais frequentes\")\n",
        "\n",
        "  # rotate x-axis labels\n",
        "  plt.xticks(rotation=90)\n",
        "\n",
        "  # save figure\n",
        "  filename = f\"{filename}.png\"\n",
        "  plt.savefig(os.path.join(WORK_DIR, filename))\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "E3yoHBhVTtSf"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_frequency_wordcloud(df, colname,  filename):\n",
        " # split the text column into words\n",
        "  df['words'] = df[colname].str.split()\n",
        "\n",
        "  # get the frequency of each word\n",
        "  word_freq = df['words'].explode().value_counts()\n",
        "\n",
        "  # get the 20 most frequent words\n",
        "  most_common_words = word_freq.head(20)\n",
        "\n",
        "  # stop words\n",
        "  stop_words = nltk.corpus.stopwords.words('portuguese') + custom_stop_words\n",
        "  # create a wordcloud object\n",
        "  wordcloud = WordCloud(width = 500, height = 500, \n",
        "                background_color ='white', \n",
        "                colormap='viridis',\n",
        "                stopwords = set(stop_words), \n",
        "                min_font_size = 10).generate_from_frequencies(most_common_words)\n",
        "\n",
        "  # plot the wordcloud\n",
        "  plt.figure(figsize = (5, 5), facecolor = None) \n",
        "  plt.imshow(wordcloud) \n",
        "  plt.axis(\"off\") \n",
        "  plt.tight_layout(pad = 0) \n",
        "\n",
        "  # save figure\n",
        "  filename = f\"{filename}.png\"\n",
        "  plt.savefig(os.path.join(WORK_DIR, filename))\n",
        "  \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ydEiLFOPONuv"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequency_barchart(df, 'documents', 'preprocessed_v1_frequency')"
      ],
      "metadata": {
        "id": "AhT9aChmPZ9E"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequency_barchart(df2, 'documents', 'preprocessed_v2_frequency')"
      ],
      "metadata": {
        "id": "x6BsyxFKSsYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequency_wordcloud(df, 'documents',  'preprocessed_v1_wordcloud')"
      ],
      "metadata": {
        "id": "H-TSkc2VSu4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequency_wordcloud(df2, 'documents',  'preprocessed_v2_wordcloud')"
      ],
      "metadata": {
        "id": "2xZ1ES07S4Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Github**"
      ],
      "metadata": {
        "id": "F0gliNtXRkfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ssh-keygen -t rsa -b 4096\n",
        "# Add github.com to our known hosts\n",
        "! ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "# Restrict the key permissions, or else SSH will complain.\n",
        "! chmod go-rwx /root/.ssh/id_rsa"
      ],
      "metadata": {
        "id": "yVd1eoa0zK7f",
        "outputId": "2b5d84bd-51b0-4e94-964a-96c271a24b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_rsa): \n",
            "Created directory '/root/.ssh'.\n",
            "Enter passphrase (empty for no passphrase): \n",
            "Enter same passphrase again: \n",
            "Your identification has been saved in /root/.ssh/id_rsa.\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub.\n",
            "The key fingerprint is:\n",
            "SHA256:OZtriREUXfX6x4FXhpkVU3wXtvs4bbDfy64fWTcRtBE root@63b3c6ac69cd\n",
            "The key's randomart image is:\n",
            "+---[RSA 4096]----+\n",
            "|      .o .... .EB|\n",
            "|      . .    ..=X|\n",
            "|     .        =+=|\n",
            "|      .  .   ...+|\n",
            "|       .S   ...=o|\n",
            "|      .  +   ..*B|\n",
            "|       oo.    =o*|\n",
            "|      . o.    .=o|\n",
            "|       ..    .+=+|\n",
            "+----[SHA256]-----+\n",
            "# github.com:22 SSH-2.0-babeld-a17d9d27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "id": "_z-PSHRzz0PY",
        "outputId": "fd2f04e0-6be1-4ce3-b545-da3c2b7894ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDN54jogPMUo/TVjm9LseTw9EpXDjeTp0XA+QIU1e07HwGU82HqToNYkarDz3dOTE1Mnm5BifuuMe0NmHrMxWgJi0RBaDIX2z+KaW1nE8oD9R661HJ4lLgutArAJOWp9dij6SJiB7hX82DXFLWmopZBEYgaxdB6aca/LsIU8xQCc/xjmS78uiBqjv7KHFQkjPCrhmwIEOtMSl7ye/zlBhFMXoHLa1xlSN8ZMlVccA3Ogl13MMdnTshPhYD3/LW3lcUU829AYzwOjvm53v5Zwa8M6AJLAg8VAslUN/xSxIs8I0AXdf2LtQWd0waV3MQFptG24OCwi9GxLexortXYyAWi4UG6HFSrelmDlF+gRh58ZYnRa+7sZWyzq3RfDkxJN+Ny5v8tGluY+twkLdZmTTLP6emsjVyhiodbDZLxJ17f8G+uv7rrkHrZg7w57rr3qtSmZ5JTHxwTt43qhl061mozCFqLhpvKC3Njf2//dFIMRsVexPKZLeu6OBPBR55fhJgwr6/u0pIWkxHp5q6BfwyqqdJ2ZMgvGJq+0d+RxRHcJgQd3jRhIg9pioaDketbapp0lQeUvQztwv92WtiScicBuI2jXYnz3w5jWEKgD/aU2MKAtBbWEmAV696ZOtOL46kznDxZynvcZNOwVWTbiSVoXhSjdN6HWwnu/+PJ5Ou6ow== root@63b3c6ac69cd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"mattheus_ribeiro@outlook.com\"\n",
        "!git config --global user.name \"punkmic\""
      ],
      "metadata": {
        "id": "4iixpGcjzxrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh -T git@github.com"
      ],
      "metadata": {
        "id": "bcqhgkdaz3QI",
        "outputId": "c8b4d852-c00d-42fb-c226-b762086436bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added the RSA host key for IP address '140.82.113.4' to the list of known hosts.\r\n",
            "Hi punkmic! You've successfully authenticated, but GitHub does not provide shell access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone git@github.com:punkmic/Topic-Modeling-Reclame-Aqui.git"
      ],
      "metadata": {
        "id": "zSA4ju5NDpqK",
        "outputId": "35f15e94-9032-45ee-9ad8-e6bc5c431cfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Topic-Modeling-Reclame-Aqui'...\n",
            "remote: Enumerating objects: 16347, done.\u001b[K\n",
            "remote: Counting objects: 100% (2166/2166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1793/1793), done.\u001b[K\n",
            "remote: Total 16347 (delta 345), reused 2149 (delta 338), pack-reused 14181\u001b[K\n",
            "Receiving objects: 100% (16347/16347), 129.25 MiB | 18.40 MiB/s, done.\n",
            "Resolving deltas: 100% (1493/1493), done.\n",
            "Checking out files: 100% (14816/14816), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Topic-Modeling-Reclame-Aqui/"
      ],
      "metadata": {
        "id": "2eEK0j1L6b7p",
        "outputId": "45b9179c-5f5b-49eb-d023-325e32876451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Topic-Modeling-Reclame-Aqui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ./datasets/preprocessed_top_20_words_removed.csv ./results/top_20_frequent_words.png ./results/top_20_frequent_words_removed.png"
      ],
      "metadata": {
        "id": "CGqBhuS6z7WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "eyMFQF889jtB",
        "outputId": "1eab65e2-facd-4c68-aaab-45a51161b574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mnew file:   datasets/preprocessed_top_20_words_removed.csv\u001b[m\n",
            "\t\u001b[32mnew file:   results/top_20_frequent_words.png\u001b[m\n",
            "\t\u001b[32mnew file:   results/top_20_frequent_words_removed.png\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Adding frequent words plot\""
      ],
      "metadata": {
        "id": "PFY8B1Ikz-U-",
        "outputId": "9a3d0ecf-d36a-4dbd-d199-6bb6f2aa51df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master 2b8e09f] Adding frequent words plot\n",
            " 3 files changed, 10511 insertions(+)\n",
            " create mode 100644 datasets/preprocessed_top_20_words_removed.csv\n",
            " create mode 100644 results/top_20_frequent_words.png\n",
            " create mode 100644 results/top_20_frequent_words_removed.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin master"
      ],
      "metadata": {
        "id": "uEJWirD70EiH",
        "outputId": "74f493f5-9cab-4dee-d94a-370703c1c0eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added the RSA host key for IP address '140.82.112.3' to the list of known hosts.\n",
            "Counting objects: 6, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (6/6), done.\n",
            "Writing objects: 100% (6/6), 1.91 MiB | 3.22 MiB/s, done.\n",
            "Total 6 (delta 2), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To github.com:punkmic/Topic-Modeling-Reclame-Aqui.git\n",
            "   95552a0..2b8e09f  master -> master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this command to push a new version of this notebook in case you have saved the notebook in github and it is outdate \n",
        "!git stash\n",
        "!git pull\n",
        "!git stash pop"
      ],
      "metadata": {
        "id": "pKF8mcLoIU3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.ssh/"
      ],
      "metadata": {
        "id": "awCNnf2Qlu_5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}