{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/unsupervised-Sentiment-Analysis---Comparisen-analysis/blob/master/Unsupervised_Sentiment_Analysis_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXD6WQspxJWc"
      },
      "source": [
        "# **Intro**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-593AM2lxMMh"
      },
      "source": [
        "## **Install Dependecies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C7gQcM_NxE2w"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# install dependecies here\n",
        "#!pip install langdetect  # for language detection\n",
        "#!pip install diagrams # for visualize the workflow\n",
        "#!pip install graphviz # for visualize the workflow\n",
        "#!pip install Pillow # for image manipulation\n",
        "!pip install textblob # for unsupervised sentiment analysis\n",
        "!pip install wordcloud # for wordcloud plot\n",
        "!pip install matplotlib # for plot\n",
        "!pip install nltk # for natural language prepocessing\n",
        "!pip install enelvo # for fix slangs, abbreviations, spelling errors\n",
        "!pip install gensim # for topic modeling \n",
        "!pip install tabulate # for print as table\n",
        "!pip install transformers # for machine learning\n",
        "!pip install numpy==1.21.6 # for mathematical\n",
        "!pip install pyldavis # for model visualization\n",
        "!pip install scikit-learn # machine learning\n",
        "!pip install bertopic # topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkydqr0uxUiB"
      },
      "source": [
        "## **Load Depencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Km0RDHK_xX8V"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# load dependecies here\n",
        "#from langdetect import detect as dt\n",
        "#from diagrams import Diagram as dg\n",
        "#from PIL import Image\n",
        "import pandas as pd\n",
        "import os \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "from numpy import mean, median\n",
        "import itertools\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "sw = nltk.corpus.stopwords.words('portuguese')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"portuguese\")\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "wnl = WordNetLemmatizer()\n",
        "import scipy\n",
        "from scipy import spatial\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import sklearn.cluster\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from enelvo.normaliser import Normaliser\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "import warnings\n",
        "import IPython\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0cdDKUrxsq1"
      },
      "source": [
        "## **Load Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clone Github repository** "
      ],
      "metadata": {
        "id": "SdXyTWtNhrU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pzdi7uwrx8Ls",
        "outputId": "d45408c9-e08d-4292-9e65-da0af0c60121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'unsupervised-Sentiment-Analysis---Comparisen-analysis' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Files cloned from github may not automatically appear in files tab in this case right click and choose update\n",
        "# this will update our files.\n",
        "!git clone https://github.com/punkmic/unsupervised-Sentiment-Analysis---Comparisen-analysis.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #!git pull "
      ],
      "metadata": {
        "id": "Z9_q4bS6laSt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load csv file**"
      ],
      "metadata": {
        "id": "PCZ8r-JXh2FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_CSV = '/content/unsupervised-Sentiment-Analysis---Comparisen-analysis/results/web_scraping_results.csv'\n",
        "docs = pd.read_csv(PATH_TO_CSV, encoding='utf-8')['body']"
      ],
      "metadata": {
        "id": "xLkm22-Ph6_P",
        "outputId": "701b739c-160f-4f6c-ee42-bbe752e80b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'body'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-abd48dca1151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH_TO_CSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/unsupervised-Sentiment-Analysis---Comparisen-analysis/results/web_scraping_results.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_CSV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'body'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EDA**"
      ],
      "metadata": {
        "id": "t6oMhx4pkmU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of items {len(docs)}')\n",
        "print(f'Number of unique words {set(docs)}')"
      ],
      "metadata": {
        "id": "bFQkBO-KkVb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot wordcloud**"
      ],
      "metadata": {
        "id": "5R_DJGXHnZhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print currently directory\n",
        "!pwd"
      ],
      "metadata": {
        "id": "IGBXVhu1vK5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and generate a word cloud image:\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100).generate(docs[0])\n",
        "\n",
        "# Save wordcloud \n",
        "if not os.path.exists(\"wordclouds/raw/\"):\n",
        "  os.makedirs(\"wordclouds/raw/\")\n",
        "wordcloud.to_file('wordclouds/raw/body_wordcloud.png')\n",
        "\n",
        "# Display wordcloud\n",
        "plt.figure()\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BDpeZcXDp5A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvlnEo8NT5mW"
      },
      "source": [
        "## **Text Pre-Processing**\n",
        "\n",
        "Guide\n",
        "* Lower Case conversion\n",
        "* Removing Punctuations\n",
        "* Stop Words Removal\n",
        "* Rare Words Removal\n",
        "* Spelling correction\n",
        "* Tokenization\n",
        "* Lemmatization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Apply enelvo - Normalize noisy words, lowercase the words and remove punctuation.**\n",
        "Enelvo is a tool for normalising noisy words in user-generated content written in Portuguese -- such as tweets, blog posts, and product reviews. It is capable of identifying and normalising spelling mistakes, internet slang, acronyms, proper nouns, and others."
      ],
      "metadata": {
        "id": "vOsQEJxm3fHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Process text**"
      ],
      "metadata": {
        "id": "ZZy2_IxfR4FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "def preprocess_data(doc_set, language):\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create a set of stop words\n",
        "    mystopwords = set(stopwords.words(\"english\"))\n",
        "    # Create s_stemmer of class SnowballStemmer\n",
        "    s_stemmer = SnowballStemmer(language = language)\n",
        "    # create a normaliser instance for portuguese language\n",
        "    if language == 'portuguese':\n",
        "      norm = Normaliser(tokenizer='readable', sanitize=True)\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        # remove numbers\n",
        "        raw = re.sub(r'\\d+', '', raw)\n",
        "         # spelling correction\n",
        "        if language == 'portuguese':\n",
        "          raw = norm.normalise(raw)\n",
        "        # tokenize words\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens \n",
        "        stopped_tokens = [token for token in tokens if token not in mystopwords and\n",
        "                            not token.isdigit() and token not in punctuation]\n",
        "        # remove short words\n",
        "        stopped_tokens = [i for i in stopped_tokens if len(i) > 2]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [s_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "Pv-EyfLkltdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_clean = preprocess_data(docs, 'english')\n",
        "print(f'Before cleaning: {len(docs)}')\n",
        "print(f'After cleaning: {len(doc_clean)}')\n",
        "print()\n",
        "print(f'Documents before cleaning: {docs[0]}')\n",
        "print()\n",
        "print(f'Documents after cleaning: {doc_clean[0]}')"
      ],
      "metadata": {
        "id": "qvC_VTbn1NZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature engineer**"
      ],
      "metadata": {
        "id": "V1Lig08yQb5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Count vectors**"
      ],
      "metadata": {
        "id": "KYE3hXvLQhgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_as_count(doc_clean):\n",
        "  cv = CountVectorizer()\n",
        "  cv.fit(doc_clean)\n",
        "  cv_tedfeatures = cv.transform(doc_clean)\n",
        "  print(f\"samples: {cv_tedfeatures.shape[0]}, features: {cv_tedfeatures.shape[1]}\")\n",
        "  print()\n",
        "  df_bow_sklearn = pd.DataFrame(cv_tedfeatures.toarray(),columns=cv.get_feature_names_out())\n",
        "  df_bow_sklearn.head()\n",
        "  return cv, cv_tedfeatures"
      ],
      "metadata": {
        "id": "ORQCGiXQQj03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF-IDF Vectors**"
      ],
      "metadata": {
        "id": "5pSmJG0ZQp5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_as_tfidf(doc_clean):\n",
        "  tv = TfidfVectorizer()\n",
        "  tv.fit(doc_clean)\n",
        "  tv_tedfeatures = tv.transform(doc_clean)\n",
        "  print(f\"samples: {tv_tedfeatures.shape[0]}, features: {tv_tedfeatures.shape[1]}\")\n",
        "  print()\n",
        "  # convert sparse matrix to dense\n",
        "  dense = tv_tedfeatures.todense()\n",
        "  denselist = dense.tolist()\n",
        "  tfid_df = pd.DataFrame(denselist,columns=tv.get_feature_names_out())\n",
        "  tfid_df.head()\n",
        "  return tv, tv_tedfeatures"
      ],
      "metadata": {
        "id": "_lFOFfHtQwU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **K-means Clustering**"
      ],
      "metadata": {
        "id": "MFLEVhepSWRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_elbow(vector_features):# Elbow method \n",
        "  elbow_method = {}\n",
        "  for k in range(1, 10):\n",
        "    kmeans_elbow = KMeans(n_clusters=k).fit(vector_features)\n",
        "    elbow_method[k] = kmeans_elbow.inertia_\n",
        "  plt.figure()\n",
        "  plt.plot(list(elbow_method.keys()), list(elbow_method.values()))\n",
        "  plt.xlabel(\"Number of cluster\")\n",
        "  plt.ylabel(\"SSE\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9G-svf2fSYWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_silhouette_score(vector_features):\n",
        "  # Silhouette method \n",
        "  for n_cluster in range(2, 10):\n",
        "    kmeans = KMeans(n_clusters=n_cluster).fit(cv_tedfeatures)\n",
        "    label = kmeans.labels_\n",
        "    sil_coeff = silhouette_score(cv_tedfeatures, label, metric='euclidean')\n",
        "    print(f\"For n_clusters={n_cluster}, The Silhouette Coefficient is {sil_coeff}\")"
      ],
      "metadata": {
        "id": "dLLUAmnkrZfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Count Vectors as Features**"
      ],
      "metadata": {
        "id": "DU-zc2NWvuqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv, cv_tedfeatures = vectorize_as_count(docs)\n",
        "plot_elbow(cv_tedfeatures)"
      ],
      "metadata": {
        "id": "ujy_ZYkzTvoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_silhouette_score(cv_tedfeatures)"
      ],
      "metadata": {
        "id": "6w42Aobdrn0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF-IDF as Features**"
      ],
      "metadata": {
        "id": "oq46GanQvzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tv, tv_tedfeatures = vectorize_as_tfidf(docs)\n",
        "plot_elbow(tv_tedfeatures)"
      ],
      "metadata": {
        "id": "ZHos6kBRr6NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_silhouette_score(tv_tedfeatures)"
      ],
      "metadata": {
        "id": "RFEAjV1qr7rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clustering Mode**l"
      ],
      "metadata": {
        "id": "eFjiC7sQV6wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define how many clusters K-means will generate\n",
        "num_topics = 2\n",
        "RANDOM_STATE = 20\n",
        "segments = KMeans(n_clusters=num_topics).fit(tv_tedfeatures)\n",
        "cluster_ids, cluster_sizes = np.unique(segments.labels_, return_counts=True)\n",
        "print(f\"Number of cluster {segments.n_clusters} \\nNumber of elements asigned to each cluster: {cluster_sizes} \")"
      ],
      "metadata": {
        "id": "wK05Apb3V8Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = KMeans(n_clusters=2)\n",
        "segments.fit(tv_tedfeatures)\n",
        "clusters = segments.labels_\n",
        "#segment outputs\n",
        "output = segments.labels_.tolist()\n",
        "ted_segmentaion = {'text': docs, 'cluster': output}\n",
        "output_df = pd.DataFrame(ted_segmentaion)\n",
        "#talks per segment\n",
        "output_df['cluster'] = segments.labels_.tolist()\n",
        "output_df['cluster'].value_counts()\n",
        "\n",
        "if not os.path.exists(\"wordclouds/clusters/\"):\n",
        "    os.makedirs(\"wordclouds/clusters/\")\n",
        "\n",
        "num_clusters = 2\n",
        "for index in range(num_clusters):\n",
        "  cluster = output_df[output_df.cluster == index]\n",
        "  #wordcloud = WordCloud(width = 1000, height = 500,collocations = False).generate_from_text(' '.join(cluster['text']))"
      ],
      "metadata": {
        "id": "4VhqQ5jBWebb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# initialize PCA with 2 components\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "# pass our X to the pca and store the reduced vectors into pca_vecs\n",
        "pca_vecs = pca.fit_transform(tv_tedfeatures.toarray())\n",
        "# save our two dimensions into x0 and x1\n",
        "x0 = pca_vecs[:, 0]\n",
        "x1 = pca_vecs[:, 1]\n",
        "# assign clusters and pca vectors to our dataframe \n",
        "output_df['x0'] = x0\n",
        "output_df['x1'] = x1"
      ],
      "metadata": {
        "id": "Us7D5fhzTGh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df.head()"
      ],
      "metadata": {
        "id": "WHKqWZNPTkFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_keywords(n_terms):\n",
        "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
        "    df = pd.DataFrame(tv_tedfeatures.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
        "    terms = tv.get_feature_names_out() # access tf-idf terms\n",
        "    for i,r in df.iterrows():\n",
        "        print('\\nCluster {}'.format(i))\n",
        "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
        "            \n",
        "get_top_keywords(10)"
      ],
      "metadata": {
        "id": "JvwUMKyiTqfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic Modeling**"
      ],
      "metadata": {
        "id": "K9wGbS-xFxkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create a dictionary using the bag of words model**\n",
        "- Document: some text.\n",
        "- Corpus: a collection of documents."
      ],
      "metadata": {
        "id": "5gjWR5iNpJ8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_corpus(doc_clean):\n",
        "    # associate each word in the corpus with a unique integer ID.\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    \n",
        "    # create the bag-of-word representation for documents (corpus)\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "    return dictionary,doc_term_matrix"
      ],
      "metadata": {
        "id": "4uHZQUypKZ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(dir, cluster_id, model, passes, iterations):\n",
        "   # Save models so they aren't lost\n",
        "  if not os.path.exists(f\"{dir}model_{iterations}i{passes}p_cluster{cluster_id}\"):\n",
        "    os.makedirs(f\"{dir}model_{iterations}i{passes}p_cluster{cluster_id}\")\n",
        "\n",
        "  model.save(f\"{dir}model_{iterations}i{passes}p_cluster{cluster_id}/model_{iterations}i{passes}p.model\")\n",
        "    \n",
        "  print(f'Model saved at: {dir}model_{iterations}i{passes}p_cluster{cluster_id}/model_{iterations}i{passes}p.model')"
      ],
      "metadata": {
        "id": "1-tO450WpRQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_topics_plot(dir, cluster_id, model, doc_clean):\n",
        "  dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
        "  vis = gensimvis.prepare(model, doc_term_matrix, dictionary)\n",
        "  \n",
        "  if not os.path.exists(f'{dir}'):\n",
        "    os.makedirs(f'{dir}')\n",
        "  pyLDAvis.save_html(vis, f'{dir}topics_cluster_{index}.html')\n",
        "  print(f'Visualization saved at: {dir}topics_cluster_{index}.html')"
      ],
      "metadata": {
        "id": "LtXScEVvpdGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Latent Dirichlet Allocation (LDA) model**"
      ],
      "metadata": {
        "id": "-jKqOvBOpWBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models\n",
        "from gensim import corpora\n",
        "from gensim.models.callbacks import PerplexityMetric, ConvergenceMetric, CoherenceMetric\n",
        "\n",
        "def create_gensim_lda_model(doc_clean, number_topics, words, iterations, passes):\n",
        "  \n",
        "  dictionary, doc_term_matrix = prepare_corpus(doc_clean) \n",
        "\n",
        "  # train model\n",
        "  lda_model = models.ldamodel.LdaModel(doc_term_matrix,\n",
        "            id2word=dictionary,\n",
        "            num_topics=num_topics,\n",
        "            iterations=iterations,\n",
        "            passes=passes)\n",
        "  \n",
        "  print(lda_model.print_topics(num_topics=number_topics, num_words=words))\n",
        "\n",
        "  return lda_model"
      ],
      "metadata": {
        "id": "9lUsZjZRpcbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        model = models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary) \n",
        "        model_list.append(model)\n",
        "        coherencemodel = models.CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "Zegq5ILop1AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(doc_clean, start, stop, step):\n",
        "    dictionary, doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    model_list, coherence_values = compute_lsa_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
        "                                                            stop, start, step)\n",
        "    # Show graph\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uC7HXbmYp5nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 100\n",
        "passes = 2\n",
        "words=10\n",
        "\n",
        "for index in range(kmeans.n_clusters):\n",
        "  cluster = output_df[output_df.cluster == index]\n",
        "  doc_clean = preprocess_data(cluster.text, 'english')\n",
        "  model = create_gensim_lda_model(doc_clean, num_topics, words, iterations, passes)\n",
        "  save_model('content/models/lda/', index, model, passes, iterations)\n",
        "\n",
        "  # plot coherence score\n",
        "  start,stop,step=2,12,1\n",
        "  plot_graph(doc_clean, start, stop, step)\n",
        "\n",
        "  # generate topic plot and save it\n",
        "  save_topics_plot('content/topics/lda/', index, model, doc_clean)\n",
        "\n"
      ],
      "metadata": {
        "id": "3RzyHfXam4EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show cluster 1 topics\n",
        "IPython.display.HTML(filename='topics/lda/topics_cluster_0.html')"
      ],
      "metadata": {
        "id": "VlvAplOzdbYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Latent Semantic Analysis (LSA) model** "
      ],
      "metadata": {
        "id": "no16baSgggQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gensim_lsa_model(doc_clean, numb_topics, words):\n",
        "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
        "    lsamodel = models.LsiModel(doc_term_matrix, num_topics=numb_topics, id2word = dictionary)  \n",
        "    print(lsamodel.print_topics(num_topics=numb_topics, num_words=words))\n",
        "    return lsamodel"
      ],
      "metadata": {
        "id": "TJ21SFj4xTse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_lsa_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        model = models.LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  \n",
        "        model_list.append(model)\n",
        "        coherencemodel = models.CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "wlZqIuNAXv9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_lsa_graph(doc_clean, start, stop, step):\n",
        "    dictionary, doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    model_list, coherence_values = compute_lsa_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
        "                                                            stop, start, step)\n",
        "    # Show graph\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A2vPCMNZYDIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 100\n",
        "passes = 2\n",
        "words=10\n",
        "for index in range(kmeans.n_clusters):\n",
        "  cluster = clusters_df[clusters_df.cluster == index]\n",
        "  doc_clean = preprocess_data(cluster.body)\n",
        "  model = create_gensim_lsa_model(doc_clean, num_topics, words)\n",
        "  save_model('content/models/lsa/', index, model, passes, iterations)\n",
        "\n",
        "  # plot coherence score\n",
        "  start,stop,step=2,12,1\n",
        "  plot_lsa_graph(doc_clean, start, stop, step)\n",
        "\n",
        "  # generate topic plot and save it\n",
        "  #save_topics_plot('topics/lsa/', index, model)"
      ],
      "metadata": {
        "id": "fYwoXXytYKnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hierarchical Dirichlet Process, HDP** "
      ],
      "metadata": {
        "id": "2MsWv_wTpwkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gensim_hdp_model(doc_clean, num_topics, words):\n",
        "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
        "    model = models.HdpModel(doc_term_matrix, id2word=dictionary)\n",
        "    return model"
      ],
      "metadata": {
        "id": "jeDgAS3ApyIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hdp_coherence_values(doc_clean):\n",
        "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
        "    model = models.HdpModel(doc_term_matrix, id2word=dictionary)\n",
        "    coherencemodel = models.CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_value = coherencemodel.get_coherence()\n",
        "    return model, coherence_value"
      ],
      "metadata": {
        "id": "y3evON-Wr21x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = 10\n",
        "for index in range(kmeans.n_clusters):\n",
        "  #cluster = clusters_df[clusters_df.cluster == index]\n",
        "  #doc_clean = preprocess_data(cluster.body)\n",
        "  model = create_gensim_hdp_model(doc_clean, num_topics, words)\n",
        "  save_model('content/models/hdp/', index, model, 'na', 'na')\n",
        "  print(model.show_topics())\n",
        "\n",
        "\n",
        "  compute_hdp_coherence_values(doc_clean)\n",
        "\n",
        "  # generate topic plot and save it\n",
        "  #save_topics_plot('content/topics/hdp/', index, model, doc_clean)"
      ],
      "metadata": {
        "id": "-94jJm38jkOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show cluster 1 topics\n",
        "IPython.display.HTML(filename='topics/hdp/topics_cluster_0.html')"
      ],
      "metadata": {
        "id": "YH26WANemB5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bertopic**"
      ],
      "metadata": {
        "id": "hOW7pfz_pBR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic"
      ],
      "metadata": {
        "id": "hJQ2bMpepFhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model \n",
        "model = BERTopic(verbose=False, language='english')"
      ],
      "metadata": {
        "id": "ZsFnQ8NfpTmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "topics, probabilities = model.fit_transform(docs)"
      ],
      "metadata": {
        "id": "XaPjc1X9paFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(topics)"
      ],
      "metadata": {
        "id": "1wLfH6w0pl1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the topic frequency\n",
        "model.get_topic_freq().head(11)"
      ],
      "metadata": {
        "id": "8xjAgIKlpwm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get one topic\n",
        "model.get_topic(0)"
      ],
      "metadata": {
        "id": "V4yYtj0cp_6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart(topics[:10])"
      ],
      "metadata": {
        "id": "HOXmtuz3qEL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Topic similarities**"
      ],
      "metadata": {
        "id": "xRorD6aVqdeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_heatmap(topics)"
      ],
      "metadata": {
        "id": "ud1yYky2qgc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bertopic prediction**"
      ],
      "metadata": {
        "id": "uvli6puNrd43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs = model.transform(new_docs)"
      ],
      "metadata": {
        "id": "-EkziqnQrhzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Texblob**"
      ],
      "metadata": {
        "id": "JwHAzXHs_bj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "Qc-jhVX2_fIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sid = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "xfpTn8qmIhO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blob_sentiment(sentence):\n",
        "  blob = TextBlob(sentence).sentiment\n",
        "  return blob.polarity"
      ],
      "metadata": {
        "id": "qJLW4tDG_5Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vader**"
      ],
      "metadata": {
        "id": "L5z5Tr7FIKBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "GBONwIhIILyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vader_sentiment(sentence):\n",
        "  vader = sid.polarity_scores(sentence)\n",
        "  return vader['compound']"
      ],
      "metadata": {
        "id": "FTBht6U6I9YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['TextBlob'] = df['body'].apply(lambda sentence: get_blob_sentiment(sentence))\n",
        "df['Vader'] = df['body'].apply(lambda sentence: get_vader_sentiment(sentence))"
      ],
      "metadata": {
        "id": "vUoKYgeGJWiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A negative sentiment score means \n",
        "negative sentiment, and a positive sentiment score means positive sentiment. The higher \n",
        "the absolute value of the score, the more confident the system is about it"
      ],
      "metadata": {
        "id": "vT8dYlRQLD1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "wvpqU7r1K9Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clustering sentences with K-Means**"
      ],
      "metadata": {
        "id": "fKOeVmUlL08M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.probability import FreqDist\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "q4q2KdkXL83B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zANBqSOtNerU"
      },
      "source": [
        "## **Save Models to Google Cloud Storage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ_BMRXENmIi"
      },
      "outputs": [],
      "source": [
        "# import google cloud dependencies\n",
        "#from google.colab import auth\n",
        "#import uuid # for generate a unique identification for google bucket\n",
        "# Define a project id in google cloud\n",
        "#project_id = '<project_ID>'\n",
        "\n",
        "#auth.authenticate_user()\n",
        "# configure gsutil\n",
        "## !gcloud config set project {project_id}\n",
        "# set bucket name\n",
        "##backet_name = f'sample-bucket-{uuid.uuid1()}'\n",
        "## !gsuit mb gs://{bucket_name}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0srGkyyiPHEq"
      },
      "outputs": [],
      "source": [
        "# upload model to Google Cloud Storage\n",
        "#!gsuit cp /tmp/name_of_file.txt gs://{bucket_name}/\n",
        "\n",
        "# location of model\n",
        "#download_location = f\"https://console.cloud.google.com/storage/browser?project={project_id}\"\n",
        "\n",
        "# donwload model from Google Cloud Storage\n",
        "#!gsuit cp gs://{bucket_name}/{filename} {download_location}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sStwWfuAzGZ3"
      },
      "source": [
        "## **References**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UcEjjp6F266"
      },
      "source": [
        "[LangDetect](https://pypi.org/project/langdetect/) <br/>\n",
        "[Diagrams](https://pypi.org/project/diagrams/) <br/>\n",
        "[Graphviz](https://pypi.org/project/graphviz/) <br/>\n",
        "[Beautifulsoap4](https://pypi.org/project/beautifulsoup4/) <br/>\n",
        "[OpLexicon](https://www.inf.pucrs.br/linatural/wordpress/recursos-e-ferramentas/oplexicon/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('.env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "6870af64cccb2dc4285885e4363d49c25adb22669b810ecb9f48ab42b19689bd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}